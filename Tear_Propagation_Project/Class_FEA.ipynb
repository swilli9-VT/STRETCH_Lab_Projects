{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfea5aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nbimporter\n",
    "import util_func\n",
    "\n",
    "class Abaqus:\n",
    "    def __init__(self, job_name, job_directory, data_directory, BC_file = False):\n",
    "        # Initialize class variables with file name strings\n",
    "        self.job = job_name\n",
    "        self.job_path = r\"{folder}\\{job}\".format(folder=job_directory,job=self.job)\n",
    "        self.data_path = r\"{folder}\\{job}\".format(folder=data_directory,job=self.job)\n",
    "\n",
    "        # Read Abaqus input file for model\n",
    "        inp_file = r\"{path}.inp\".format(path = self.job_path)\n",
    "        with open(inp_file, \"r\") as file:\n",
    "            inp = [line.rstrip() for line in file]\n",
    "            file.seek(0)\n",
    "            self.lines_inp = file.readlines()\n",
    "            file.close()\n",
    "\n",
    "        # Extract coordinate strings from input file\n",
    "        coord_str = tuple(i.split(',') for i in\n",
    "                          inp[(inp.index('*Node')+1):inp.index('*Element, type=C3D8R')])\n",
    "        \n",
    "        # Create class variables storing coordinates, node count, and dof count info\n",
    "        self.coord     = np.array(coord_str, dtype=float)[:,1:]\n",
    "        self.num_nodes = len(self.coord)\n",
    "        self.num_dof   = 3*self.num_nodes\n",
    "\n",
    "        # Extract connectivity matrix strings from input file\n",
    "        connect_str = tuple(i.split(',') for i in\n",
    "                            inp[inp.index('*Element, type=C3D8R')+1:inp.index('*Nset, nset=Bottom')])\n",
    "        \n",
    "        # Create class variables storing connectivity matrix and element count\n",
    "        self.connect = np.array(connect_str, dtype=int)[:,1:] - 1\n",
    "        self.num_el  = len(self.connect)\n",
    "        \n",
    "        # Declare default flag indicating boundary condition array has not yet been constructed\n",
    "        self.BC_flag = False\n",
    "        # Read in boolean array denoting boundary condition indices from file if it exists\n",
    "        if BC_file:\n",
    "            self.BC_mask = pd.read_csv(BC_file).to_numpy(dtype=bool)[:,1]\n",
    "            self.BC_flag = True # switch BC_flag to true once boundary condition array has been read in\n",
    "        \n",
    "        # Initialize list of tags common to all results files\n",
    "        common_tags = [\"beta_d\",\"beta_m\",\"beta_p\",\"t\"]\n",
    "        \n",
    "        # Initialize tags for use in specific data storage files\n",
    "        dof_tags    = [[\"u{node}\".format(node=i), \"v{node}\".format(node=i), \"w{node}\".format(node=i)] \n",
    "                        for i in range(1,self.num_nodes+1)]\n",
    "        strain_tags = [[\"E22_{node}\".format(node=i), \"E33_{node}\".format(node=i)] for i in range(1,self.num_nodes+1)]\n",
    "        beta_tags   = [[\"beta1_{node}\".format(node=i), \"beta2_{node}\".format(node=i)] for i in range(1,self.num_nodes+1)]\n",
    "        alpha_tags  = [[\"Ei_{node}\".format(node=i)] for i in range(1,self.num_nodes+1)]\n",
    "        dam_tags    = [[\"dam_{node}\".format(node=i)] for i in range(1,self.num_nodes+1)]\n",
    "        del_tags    = [[\"status_{el}\".format(el=i)] for i in range(1,self.num_el+1)]\n",
    "        \n",
    "        # Combine storage tags into single list for handling with comprehension\n",
    "        tags_list = [dof_tags, strain_tags, beta_tags, alpha_tags, dam_tags, del_tags]\n",
    "\n",
    "        # Create class variable with common tags and file specific storage tags combined as tuple elements\n",
    "        self.data_tags = tuple(common_tags + [x for xs in tags for x in xs] for tags in tags_list)\n",
    "        \n",
    "        # Create class variable containing tags for the names of the storage files themselves\n",
    "        self.file_tags = (\"Snapshots\", \"Strain\", \"Beta\", \"E_Alpha\", \"Damage\", \"Status\")\n",
    "        \n",
    "        \n",
    "    def get_stiffness(self,mu): # Function to retrieve stiffness matrices from files\n",
    "        # Initialize storage for stiffness matrix\n",
    "        Kn = np.zeros((self.num_dof, self.num_dof)) \n",
    "        \n",
    "        # Read in stiffness matrix from file\n",
    "        stif_file = r\"{path}_STIF2.mtx\".format(path = self.job_path)\n",
    "        with open(stif_file, \"r\") as file:\n",
    "            stif = [line.strip().split() for line in file]\n",
    "            file.close()\n",
    "\n",
    "        # Insert non-zero stiffness matrix elements at designated indices\n",
    "        for line in stif:\n",
    "            i, j = int(line[0])-1, int(line[1])-1\n",
    "            val = float(line[2])\n",
    "            Kn[i,j] = val\n",
    "\n",
    "        # Use stiffness matrix to determine indices of boundary conditions and store as BC_mask if not \n",
    "        # already retrieved from file \n",
    "        if not self.BC_flag:\n",
    "            BC_ind = [i for i in range(self.num_dof) if np.diag(Kn)[i] > 10**30]\n",
    "            mask = np.ones(len(Kn), dtype=bool)\n",
    "            mask[BC_ind] = False\n",
    "            \n",
    "            self.BC_mask = mask\n",
    "            \n",
    "            mask_frame = pd.DataFrame(self.BC_mask)\n",
    "            mask_frame.to_csv(r\"{path}_BC_mask.csv\".format(path=self.data_path))\n",
    "            self.BC_flag = True\n",
    "        \n",
    "        # Reduce stiffness matrix by removing rows and columns associated with boundary conditions to save memory\n",
    "        Kc_n = Kn[self.BC_mask, :]\n",
    "        Kc_n = Kc_n[:, self.BC_mask]\n",
    "        Kc_n = pd.DataFrame(Kc_n)\n",
    "        \n",
    "        # Write stiffness matrix to file\n",
    "        Kc_n.to_csv(r\"{path}_Kc_{beta_d}_{beta_m}_{beta_p}.csv\"\n",
    "                    .format(path=self.data_path, beta_d=mu[0], beta_m=mu[1], beta_p=mu[2]))\n",
    "        \n",
    "        # CSV file corruption is somewhat common with matrices of this size.\n",
    "        # Run a check on the CSV to ensure it can be read, and rewrite the CSV if it cannot.\n",
    "        corrupt = True\n",
    "        while corrupt:\n",
    "            try:\n",
    "                check = pd.read_csv(r\"{path}_Kc_{beta_d}_{beta_m}_{beta_p}.csv\"\n",
    "                    .format(path = self.data_path, beta_d=mu[0], beta_m=mu[1], beta_p=mu[2]), index_col=0)\n",
    "                corrupt = False\n",
    "            except:\n",
    "                Kc_0.to_csv(r\"{path}_Kc_{beta_d}_{beta_m}_{beta_p}.csv\"\n",
    "                    .format(path = self.data_path, beta_d=mu[0], beta_m=mu[1], beta_p=mu[2]))\n",
    "        \n",
    "        \n",
    "    def get_max_load(self, mu): # Function to retrieve FE load vector\n",
    "        \n",
    "        # This function requires the BC mask but cannot create it internally.\n",
    "        # Ensure it exists before proceeding.\n",
    "        assert self.BC_flag, \"\"\"The boundary condition mask has not been defined. \n",
    "                                Retrieve it via get_stiffness() or directly from \n",
    "                                BC_file during __init__.\"\"\"\n",
    "        \n",
    "        # Read in load vector from file\n",
    "        load_file = r\"{path}_LOAD2.mtx\".format(path = self.job_path)\n",
    "        with open(load_file, \"r\") as file:\n",
    "            load = [line.strip().split() for line in file]\n",
    "            file.close()\n",
    "\n",
    "        # Intialize load vector storage array\n",
    "        Fn = np.zeros(self.num_dof)\n",
    "\n",
    "        # Insert non-zero load vector elements at designated indices\n",
    "        for line in load[2:]:\n",
    "            i = int(line[0])-1\n",
    "            val = float(line[1])\n",
    "            Fn[i] = val\n",
    "\n",
    "        # Reduce load vector by removing elements at boundary condition indices\n",
    "        Fc_n = pd.DataFrame(Fn[self.BC_mask])\n",
    "        \n",
    "        # Write load vector to file\n",
    "        Fc_n.to_csv(r\"{path}_Fc_{beta_d}_{beta_m}_{beta_p}.csv\"\n",
    "                    .format(path=self.data_path, beta_d=mu[0], beta_m=mu[1], beta_p=mu[2]))\n",
    "    \n",
    "    \n",
    "    def init_data_storage(self):# Function for clearing and initializing data storage files\n",
    "        \n",
    "        # Create empty data storage CSVs with appropriate column labels\n",
    "        for j in range(len(self.data_tags)):\n",
    "            pd.DataFrame(columns=self.data_tags[j]).to_csv(r\"{path}_{tag}.csv\"\n",
    "                              .format(path=self.data_path, tag=self.file_tags[j]), mode=\n",
    "                                     'w')\n",
    "            \n",
    "#         # Create empty data storage text file for tear results\n",
    "#         tear_file = r\"{path}_tear_results.txt\".format(path=self.data_path)\n",
    "#         with open(tear_file, \"w+\") as file:\n",
    "#             file.close()\n",
    "        \n",
    "    def get_data(self, mu): # Funtion for retrieval of FE results data\n",
    "        \n",
    "        # This function requires the BC mask but cannot create it internally.\n",
    "        # Ensure it exists before proceeding.\n",
    "        assert self.BC_flag, \"\"\"The boundary condition mask has not been defined. \n",
    "                                Retrieve it via get_stiffness() or directly from \n",
    "                                BC_file during __init__.\"\"\"\n",
    "        \n",
    "        # Initialize values for data labeling by mu parameter case\n",
    "        mu_arr = np.array(mu)\n",
    "        \n",
    "        # Read in status file\n",
    "        sta_file = r\"{path}.sta\".format(path = self.job_path)\n",
    "        with open(sta_file, \"r\") as file:\n",
    "            sta = [line.strip().split() for line in file]\n",
    "            file.close()\n",
    "\n",
    "        # Save data regarding element deletion to dataframe\n",
    "        status = pd.DataFrame(sta[5:-2])\n",
    "        \n",
    "        # Use step increments from status file to calculate time step values and store in array\n",
    "        step_inc = np.array(tuple(status[8].loc[status[0]=='1'].loc[\n",
    "                                    ~status[2].str.contains('U')].values), dtype=float)\n",
    "        step_times = np.array([sum(step_inc[0:n]) for n in range(1,len(step_inc)+1)])\n",
    "        \n",
    "        \n",
    "        # Read .DAT to retrieve all FE results data\n",
    "        dat_file = r\"{path}.dat\".format(path = self.job_path)\n",
    "        with open(dat_file, \"r\") as file:\n",
    "            dat = [line.strip() for line in file]\n",
    "            file.close()\n",
    "        \n",
    "        # Retrieve starting and ending rows for displacement snapshots \n",
    "        U_start = np.array([i for i, x in enumerate(dat) if x == 'NODE FOOT-  U1             U2             U3'])\n",
    "        U_end = np.array([i for i, x in enumerate(dat) if \n",
    "                          (x.startswith('AT NODE') and dat[i-1].startswith('MAXIMUM'))])\n",
    "        U_start = U_start+3\n",
    "        U_end = U_end-2\n",
    "        \n",
    "        # Retrieve starting and ending rows for nodal data (state-dependent variables)\n",
    "        node_start = np.array([i for i, x in enumerate(dat) if x.startswith('NODE  FOOT-   SDV2')])\n",
    "        node_end = np.array([i for i, x in enumerate(dat) if \n",
    "                          (x.startswith('NODE') and dat[i-1].startswith('MAXIMUM'))])\n",
    "        node_start = node_start+3\n",
    "        node_end = node_end-2\n",
    "        \n",
    "        # Retrieve starting and ending rows for status of deleted elements\n",
    "        del_start = np.array([i for i, x in enumerate(dat) if x == 'ELEMENT  PT FOOT-       SDV16'])\n",
    "        del_end = np.array([i for i, x in enumerate(dat) if \n",
    "                          (x.startswith('ELEMENT') and dat[i-1].startswith('MAXIMUM'))])\n",
    "        del_start = del_start + 3\n",
    "        del_end = del_end - 2\n",
    "\n",
    "        # Initialize list of empty dataframes with column tags from class initialization in which to store results data\n",
    "        FE_data = [pd.DataFrame(columns=tags) for tags in self.data_tags]\n",
    "        \n",
    "        # Initialize a storage vector to be used for displacement snapshots\n",
    "        U = np.zeros((self.num_dof,))\n",
    "        \n",
    "        # Iterate over time steps and retrieve relevant data at each snapshot\n",
    "        for i in range(len(step_times)):\n",
    "            # Gather .DAT file slices for displacement, nodal data, and element data for current time step\n",
    "            tempU = dat[U_start[i]:U_end[i]]\n",
    "            tempN = dat[node_start[i]:node_end[i]]\n",
    "            tempE = dat[del_start[i]:del_end[i]]\n",
    "            \n",
    "            # Collect displacement, nodal, and element data into numpy arrays\n",
    "            Uc = np.array([x.split()[1:] for x in tempU], dtype=float).flatten()\n",
    "            sep_N = np.array([x.split()[1:] for x in tempN], dtype=float)\n",
    "            sep_E = np.array([x.split() for x in tempE])\n",
    "\n",
    "            # Map reduced displacement onto full displacement via BC_mask indices \n",
    "            U[self.BC_mask] = Uc\n",
    "            \n",
    "            # Separate nodal data into its components\n",
    "            E_hoop_axial = sep_N[:,:2].flatten()\n",
    "            Beta         = sep_N[:,2:4].flatten()\n",
    "            Alpha        = sep_N[:,4].flatten()\n",
    "            Dam          = sep_N[:,5].flatten()\n",
    "            \n",
    "            # Separate element dat into index and element deletion status arrays\n",
    "            stat_ind = np.array(sep_E[:,0], dtype=int) - 1\n",
    "            Status           = np.zeros((self.num_el,))\n",
    "            Status[stat_ind] = np.array(sep_E[:,2], dtype=float).flatten()\n",
    "            \n",
    "            # Place data arrays in a list for iteration\n",
    "            data_list = [U, E_hoop_axial, Beta, Alpha, Dam, Status]\n",
    "            \n",
    "            # Iterate over list of arrays and append data to corresponding dataframes\n",
    "            for j in range(len(FE_data)):\n",
    "                FE_data[j].loc[len(FE_data[j])] = np.hstack((mu_arr,step_times[i],data_list[j]))\n",
    "        \n",
    "        # Iterate over list of file tags and write each corresponding dataframe to file\n",
    "        for j in range(len(self.file_tags)):\n",
    "            FE_data[j].to_csv(r\"{path}_{tag}.csv\"\n",
    "                              .format(path=self.data_path, tag=self.file_tags[j]), mode='a', header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9d6026",
   "metadata": {},
   "outputs": [],
   "source": [
    "#         init_file = r\"{path}_INIT.inp\".format(path = self.job_path)\n",
    "#         with open(init_file, \"r\") as file:\n",
    "#             self.lines_init = file.readlines()\n",
    "#             file.close()\n",
    "            \n",
    "#         ###############################################################################\n",
    "        \n",
    "#         K0 = np.zeros((self.num_dof, self.num_dof))\n",
    "        \n",
    "#         init_stif_file = r\"{path}_INIT_STIF2.mtx\".format(path = self.job_path)\n",
    "\n",
    "#         with open(init_stif_file, \"r\") as file:\n",
    "#             stif = [line.strip().split() for line in file]\n",
    "#             file.close()\n",
    "\n",
    "#         for line in stif:\n",
    "#             i, j = int(line[0])-1, int(line[1])-1\n",
    "#             val = float(line[2])\n",
    "#             K0[i,j] = val\n",
    "            \n",
    "#         Kc_0 = K0[self.BC_mask, :]\n",
    "#         Kc_0 = Kc_0[:, self.BC_mask]\n",
    "#         Kc_0 = pd.DataFrame(Kc_0)\n",
    "        \n",
    "#         Kc_0.to_csv(r\"{path}_Kc0_{beta_d}_{beta_m}_{beta_p}.csv\"\n",
    "#                     .format(path = self.data_path, beta_d=mu[0], beta_m=mu[1], beta_p=mu[2]))\n",
    "        \n",
    "#         corrupt = True\n",
    "#         while corrupt:\n",
    "#             try:\n",
    "#                 check = pd.read_csv(r\"{path}_Kc0_{beta_d}_{beta_m}_{beta_p}.csv\"\n",
    "#                     .format(path = self.data_path, beta_d=mu[0], beta_m=mu[1], beta_p=mu[2]), index_col=0)\n",
    "#                 corrupt = False\n",
    "#             except:\n",
    "#                 Kc_0.to_csv(r\"{path}_Kc0_{beta_d}_{beta_m}_{beta_p}.csv\"\n",
    "#                     .format(path = self.data_path, beta_d=mu[0], beta_m=mu[1], beta_p=mu[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f297251f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def get_tear_len(self, mu): # Function for saving final tear results data\n",
    "        \n",
    "#         # Create file path string for tear data storage\n",
    "#         tear_log = self.job_path.replace(self.job, \"tear_log.txt\")\n",
    "\n",
    "#         # Read length of tear propagation in either direction (up or down)\n",
    "#         da_i, da_c = [float(x) for x in util_func.read_n_to_last_line(tear_log, n=5).split()]\n",
    "        \n",
    "#         # Compute total tear length at end of simulation\n",
    "#         tear_len = 3.1 + da_i + da_c\n",
    "        \n",
    "#         # Create list with beta parameters and tear length data\n",
    "#         tear_data = [mu[0], mu[1], mu[2], da_i, da_c, tear_len, '\\n']\n",
    "        \n",
    "#         # Write tear length data to file\n",
    "#         tear_file = r\"{path}_tear_results.txt\".format(path=self.data_path)\n",
    "#         with open(tear_file, \"a\") as file:\n",
    "#             file.write(' '.join(str(i) for i in tear_data))\n",
    "#             file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ROM_ML] *",
   "language": "python",
   "name": "conda-env-ROM_ML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
