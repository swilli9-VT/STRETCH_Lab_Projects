{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e10660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.special\n",
    "import nbimporter\n",
    "import util_func\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146fb392",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_Data_Prep:\n",
    "    def __init__(self, beta, FE):\n",
    "        # Pull necessary components from input variables \n",
    "        # and save as either class objects or variables depending on use-case \n",
    "        self.beta = beta\n",
    "        self.data_path = FE.data_path\n",
    "        self.BC_mask = FE.BC_mask\n",
    "        coordinates = FE.coord\n",
    "        nodes = FE.connect\n",
    "        \n",
    "        # load FE data on snapshots and element deletion into dataframes\n",
    "        snap = pd.read_csv(r\"{path}_Snapshots.csv\".format(path=self.data_path), index_col=0)\n",
    "        status = pd.read_csv(r\"{path}_Status.csv\".format(path=self.data_path), index_col=0)\n",
    "        \n",
    "        # save the full set of snapshots into numpy array for handling\n",
    "        self.y_full = snap.iloc[:, 4:].to_numpy()#[:,BC_mask]\n",
    "        \n",
    "        # Create numpy array of booleans denoting element deletion\n",
    "        stat = ~status.iloc[:, 4:].to_numpy(dtype=bool)\n",
    "        # Sum boolean array on axis 1 to determine total number of deleted elements at each snapshot\n",
    "        stat_sum = np.sum(stat, axis=1)\n",
    "        \n",
    "        # Declare unit vecotr pointing to axial midline of tear in x-y plane\n",
    "        unit_vec = np.array([1/np.sqrt(2), 1/np.sqrt(2)])\n",
    "        \n",
    "        # Initialize storage lists for indices of nodes on axial mid line both above and below mid-height\n",
    "        mid_line_above = []\n",
    "        mid_line_below = []\n",
    "        # Iterate over coordinates to search for desired nodes\n",
    "        for i in range(len(coordinates)):\n",
    "            c = coordinates[i,:2] #coordinate vector in x-y plane\n",
    "            unit_c = c/np.linalg.norm(c) # take unit vector of coordinate vector\n",
    "            \n",
    "            # check if coordinate unit vector is near identical to reference unit vector\n",
    "            if np.linalg.norm(unit_c-unit_vec)<0.01:\n",
    "                if FE.coord[i, 2] > 7:\n",
    "                    mid_line_above.append(i)\n",
    "                else:\n",
    "                    mid_line_below.append(i)\n",
    "\n",
    "            # Save nodes indices of coordinates on lateral edges of tear\n",
    "            if np.linalg.norm(coordinates[i,:] - [3.81213, 3.38787, 7])<0.01:\n",
    "                min_left = i\n",
    "            if np.linalg.norm(coordinates[i,:] - [3.38787, 3.81213, 7])<0.01:\n",
    "                min_right = i\n",
    "\n",
    "        # sort indices into numpy array by distance from mid-height\n",
    "        ind_above = np.argsort(coordinates[mid_line_above,2]-7)\n",
    "        ind_below = np.argsort(coordinates[mid_line_below,2]-7)\n",
    "\n",
    "        # Select coordinates in ordered array arranged by distance from mid-height\n",
    "        coord_above = coordinates[np.array(mid_line_above)[ind_above]]\n",
    "        coord_below = coordinates[np.array(mid_line_below)[ind_below]]\n",
    "\n",
    "        # Reduce indices to inlcude only nodes on the midline which are in the middle of the geomtry's through-thickness\n",
    "        nodes_above = []\n",
    "        for i in range(3, len(coord_above)+1, 3):\n",
    "            norm_set = [np.linalg.norm(coord_above[i-3:i][k]) for k in range(3)]\n",
    "            nodes_above.append(ind_above[i-3:i][np.argwhere(norm_set == np.median(norm_set)).flatten()])\n",
    "        nodes_below = []\n",
    "        for i in range(3, len(coord_below)+1, 3):\n",
    "            norm_set = [np.linalg.norm(coord_below[i-3:i][k]) for k in range(3)]\n",
    "            nodes_below.append(ind_below[i-3:i][np.argwhere(norm_set == np.median(norm_set)).flatten()])\n",
    "\n",
    "        # Finalize index list and make into numpy array\n",
    "        maj_above = np.array(mid_line_above)[np.hstack(nodes_above)]\n",
    "        maj_below = np.flip(np.array(mid_line_below)[np.hstack(nodes_below)])\n",
    "\n",
    "        # Initialize counting operators and storage to gather geomtric infor about tear ellipse at each snapshot\n",
    "        num_del = 0\n",
    "        da_i = 0\n",
    "        da_c = 0\n",
    "        a_i = np.zeros((len(stat_sum), 1))\n",
    "        a_c = np.zeros((len(stat_sum), 1))\n",
    "        min_axis = np.zeros((len(stat_sum), 1))\n",
    "        area = np.zeros((len(stat_sum), 1))\n",
    "        perimeter = np.zeros((len(stat_sum), 1))\n",
    "        nu_i = np.zeros((len(stat_sum), 1))\n",
    "        theta_i = np.zeros((len(stat_sum), 1))\n",
    "        nu_c = np.zeros((len(stat_sum), 1))\n",
    "        theta_c = np.zeros((len(stat_sum), 1))\n",
    "        \n",
    "        # Iterate using array which contains total number of delted elements at each snapshot as length reference\n",
    "        for i in range(len(stat_sum)):\n",
    "            # Reshape snapshots to nodal displacement configuration\n",
    "            node_disp = self.y_full[i,:].reshape((6225,3))\n",
    "            # Compute coordinates of nodes in deformed mesh\n",
    "            disp_field = FE.coord + node_disp\n",
    "\n",
    "            # check if tear propagation has occured on current step\n",
    "            if (stat_sum[i] != num_del and stat_sum[i] % 2 == 0):\n",
    "                # find all elements which have been deleted\n",
    "                curr_del = np.argwhere(stat[i,:]).flatten()\n",
    "                # create array which determines whether said elements are above or below mid-height\n",
    "                temp = np.array([coordinates[nodes[k][0]][2] > 7 for k in curr_del])\n",
    "                \n",
    "                # update counting variables to reflect how many elements are delted above and below mid-height\n",
    "                da_i = np.sum(temp)\n",
    "                da_c = np.sum([not k for k in temp])\n",
    "                \n",
    "                # update total number of deleted elements\n",
    "                num_del = stat_sum[i]\n",
    "\n",
    "            # declare node indices corresponding to upper and lower ends of tear major axis\n",
    "            maj_ind_a = maj_above[int(da_i/2)]\n",
    "            maj_ind_b = maj_below[int(da_c/2)]\n",
    "\n",
    "            # compute vector which points to mid-height between lateral edges of tear\n",
    "            mid_height = (disp_field[min_left] + disp_field[min_right])/2\n",
    "\n",
    "            # compute upper and lower major axes of tear relative to mid-height\n",
    "            a_i[i] = np.abs(disp_field[maj_ind_a][2] - mid_height[2])\n",
    "            a_c[i] = np.abs(mid_height[2] - disp_field[maj_ind_b][2])\n",
    "            \n",
    "            # compute minor axis of tear\n",
    "            min_axis[i] = np.linalg.norm(disp_field[min_left] - disp_field[min_right])/2\n",
    "\n",
    "            # compute area of approximate ellipse projection of tear\n",
    "            area[i] = (np.pi/2)*(a_i[i]*min_axis[i] + a_c[i]*min_axis[i])\n",
    "\n",
    "            # numerically approximate perimeter of ellipse projection of tear using infinite sum taken to 10 iterations\n",
    "            h1 = (a_i[i] - min_axis[i])**2 / (a_i[i] + min_axis[i])**2\n",
    "            h2 = (a_c[i] - min_axis[i])**2 / (a_c[i] + min_axis[i])**2\n",
    "            perimeter[i] = (np.pi*(a_i[i] + min_axis[i])*np.sum([(h1**n)*scipy.special.binom(0.5, n) for n in range(10)])\n",
    "                            + np.pi*(a_c[i] + min_axis[i])*np.sum([(h2**n)*scipy.special.binom(0.5, n) for n in range(10)]))/2\n",
    "\n",
    "            # calculate angles between vectors pointing from origin to upper and lower ends of major axis \n",
    "            # and vector pointing to left end of minor axis\n",
    "            theta_i[i] = (180/np.pi)*np.arccos( np.dot(disp_field[min_left], disp_field[maj_ind_a])/\n",
    "                                   (np.linalg.norm(disp_field[min_left])*np.linalg.norm(disp_field[maj_ind_a])) )\n",
    "            theta_c[i] = (180/np.pi)*np.arccos( np.dot(disp_field[min_left], disp_field[maj_ind_b])/\n",
    "                                   (np.linalg.norm(disp_field[min_left])*np.linalg.norm(disp_field[maj_ind_b])) )\n",
    "            \n",
    "            # calculate angles vectors pointing from left end of minor axis to upper and lower ends of major axis\n",
    "            # and vector pointing from left end of minor axis to right end of minor axis\n",
    "            nu_i[i] = (180/np.pi)*np.arccos( np.dot(disp_field[min_left] - disp_field[maj_ind_a], disp_field[min_left] - disp_field[min_right])/\n",
    "                                   (np.linalg.norm(disp_field[min_left] - disp_field[maj_ind_a])*np.linalg.norm(disp_field[min_left] - disp_field[min_right])) )\n",
    "            nu_c[i] = (180/np.pi)*np.arccos( np.dot(disp_field[min_left] - disp_field[maj_ind_b], disp_field[min_left] - disp_field[min_right])/\n",
    "                                   (np.linalg.norm(disp_field[min_left] - disp_field[maj_ind_b])*np.linalg.norm(disp_field[min_left] - disp_field[min_right])) )\n",
    "        \n",
    "\n",
    "        # Construct full set of ML inputs from FE parameters and computed geometric data\n",
    "        self.x_full = np.hstack((snap.iloc[:,:3].to_numpy(), np.expand_dims(snap.iloc[:,3].to_numpy()*50, axis=1), \n",
    "                                a_i, a_c, min_axis, area, perimeter, theta_i, theta_c, nu_i, nu_c))\n",
    "        \n",
    "        \n",
    "        # Initialize storage for reduced sample sets from full data \n",
    "        samples_x = []\n",
    "        samples_y = []\n",
    "        # create counting variables to separate by beta parameter case\n",
    "        x_time = snap.iloc[:,3].to_numpy()\n",
    "        start_ind = np.sort(np.argsort(x_time)[:8])\n",
    "        stop_ind = start_ind - 1\n",
    "        stop_ind = np.hstack([stop_ind[1:], stop_ind[0]])\n",
    "        # iterate over array of counting varibales which separates beta parameter cases\n",
    "        for i in range(len(stop_ind)):\n",
    "            # select time stamps and subsets of x and y data from each beta parameter case\n",
    "            times_mu = x_time[start_ind[i]:stop_ind[i]]\n",
    "            x_mu = self.x_full[start_ind[i]:stop_ind[i],:]\n",
    "            y_mu = self.y_full[start_ind[i]:stop_ind[i],:]\n",
    "\n",
    "            # search for indices corresponding to approximately equal time steps of 0.01s (0.5 kPa)\n",
    "            down_sample = [np.argmin(np.abs(times_mu-k)) for k in [x/100 for x in range(1,101)]]\n",
    "            # ensure index uniqueness and combine into numpy array\n",
    "            down_sample = np.unique(np.hstack(down_sample))\n",
    "            \n",
    "            # add samples from subsets to sample list\n",
    "            samples_x.append(x_mu[down_sample,:])\n",
    "            samples_y.append(y_mu[down_sample,:])\n",
    "        \n",
    "        # convert complete sample list for all beta parameter cases into numpy array and save as class object\n",
    "        self.x = np.vstack(samples_x)\n",
    "        self.y = np.vstack(samples_y)\n",
    "        \n",
    "        \n",
    "        # Divide degrees of freedom into 2 subdomains using cross products to \n",
    "        # search for dofs inside or outside certatin angle sections\n",
    "        a_o = [4.71, 1.95] # left outer bound to define subdomain near tear\n",
    "        a_i = [4.29, 2.71] # left inner bound to define subdomain away from tear\n",
    "\n",
    "        b_o = a_o[::-1] # right outer bound\n",
    "        b_i = a_i[::-1] # right inner bound\n",
    "\n",
    "        # define cross products of inner and outer bounds against which to compare other vectors\n",
    "        A_ixB_i = (a_i[0]*b_i[1] - a_i[1]*b_i[0]) # always positive\n",
    "        A_oxB_o = (a_o[0]*b_o[1] - a_o[1]*b_o[0]) # always positive\n",
    "\n",
    "        # iterate over all coordinates to find which coordinate vectors belong to each subdomain\n",
    "        ind_away_from_tear = []\n",
    "        ind_near_tear = []\n",
    "        for i in range(len(coordinates)):\n",
    "            c = coordinates[i,:2] # coordinate vector in x-y plane\n",
    "\n",
    "            # coordinate vector cross products\n",
    "            A_ixC = (a_i[0]*c[1] - a_i[1]*c[0])\n",
    "            CxB_i = (c[0]*b_i[1] - c[1]*b_i[0])\n",
    "            CxA_i = (c[0]*a_i[1] - c[1]*a_i[0])\n",
    "\n",
    "            A_oxC = (a_o[0]*c[1] - a_o[1]*c[0])\n",
    "            CxB_o = (c[0]*b_o[1] - c[1]*b_o[0])\n",
    "            CxA_o = (c[0]*a_o[1] - c[1]*a_o[0])\n",
    "\n",
    "            # conditional to catch vectors outside of inner bounds\n",
    "            if not ((CxB_i * CxA_i) <= 0 and (A_ixB_i * A_ixC)>=0):\n",
    "                ind_away_from_tear.append(i)\n",
    "\n",
    "            # conditional to catch vectors inside of outer bounds\n",
    "            if ((CxB_o * CxA_o) <= 0 and (A_oxB_o * A_oxC) >=0):\n",
    "                ind_near_tear.append(i)\n",
    "\n",
    "        # iterate over connectivity matrix to determine which whole elements belong to each domain\n",
    "        nodes_away = []\n",
    "        nodes_near = []\n",
    "        for i in range(len(nodes)):\n",
    "            # search for nodes which contain indices retrieved via cross product method above\n",
    "            found_a = np.isin(nodes[i,:], ind_away_from_tear)\n",
    "            found_n = np.isin(nodes[i,:], ind_near_tear)\n",
    "            \n",
    "            # flag elements where all nodes reside in the subdomain bounds\n",
    "            flag_a = np.sum(found_a) == 8\n",
    "            flag_n = np.sum(found_n) == 8\n",
    "\n",
    "            # add elements to subdomain set according to flags\n",
    "            if (flag_a & ~flag_n):\n",
    "                nodes_away.append(nodes[i,:])\n",
    "\n",
    "            elif (flag_n & ~flag_a):\n",
    "                nodes_near.append(nodes[i,:])\n",
    "\n",
    "            elif (flag_a & flag_n):\n",
    "                nodes_away.append(nodes[i,:])\n",
    "                nodes_near.append(nodes[i,:])\n",
    "\n",
    "        # combine subdomain nodes into numpy arrays\n",
    "        nodes_away = np.vstack(nodes_away)\n",
    "        nodes_near = np.vstack(nodes_near)\n",
    "\n",
    "        # convert from node sets to dof sets for each subdomain\n",
    "        dof_away = np.array([[3*x, 3*x+1, 3*x+2] for x in np.unique(nodes_away)]).flatten()\n",
    "        dof_near = np.array([[3*x, 3*x+1, 3*x+2] for x in np.unique(nodes_near)]).flatten()\n",
    "\n",
    "        # nodes_schwarz1 = (np.sum(np.isin(nodes_away, nodes_near), axis=1) == nodes_away.shape[1]).flatten()\n",
    "        # nodes_schwarz2 = (np.sum(np.isin(nodes_near, nodes_away), axis=1) == nodes_away.shape[1]).flatten()\n",
    "\n",
    "        # save subdomain dof sets for later use\n",
    "        self.y_omega1 = self.y[:, dof_away]\n",
    "        self.y_omega2 = self.y[:, dof_near]\n",
    "        \n",
    "        # save boolean masks which identify dofs that lie on the overlap region in each subdomain\n",
    "        self.schwarz_mask1 = np.isin(dof_away, dof_near)\n",
    "        self.schwarz_mask2 = np.isin(dof_near, dof_away) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e928f27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#             stat_mu = ~status_subset.iloc[:, 4:].to_numpy(dtype=bool)\n",
    "#             stat_sum = np.sum(stat_mu, axis=1)\n",
    "#             val = np.unique(stat_sum)\n",
    "#             val = val[val % 2 == 0][1:]\n",
    "\n",
    "#             if enrich:\n",
    "#                 prop_snap = [np.min(np.where(stat_sum==j)) for j in val]\n",
    "#                 times_enrich = status_subset['t'].iloc[prop_snap].to_numpy()\n",
    "\n",
    "#                 for c in range(len(times_enrich)):\n",
    "#                     start = np.argmin(np.abs(times_mu-(times_enrich[c]-0.005)))\n",
    "#                     end = np.argmin(np.abs(times_mu-(times_enrich[c]+0.005)))\n",
    "#                     step = int(np.round((end-start)/30))\n",
    "#                     down_sample.append(np.array(range(start,end,step)))\n",
    "\n",
    "\n",
    "#     def get_POD_basis(self, A):\n",
    "#         covariance = np.matmul(np.transpose(A),A)\n",
    "\n",
    "#         L, V = np.linalg.eig(covariance)\n",
    "#         Lambda = np.sort(np.real(L))[::-1]\n",
    "#         L_arg = np.argsort(np.real(L))[::-1]\n",
    "\n",
    "#         phi = np.real(V)[:, L_arg]\n",
    "\n",
    "#         sigma = np.sqrt(Lambda)\n",
    "#         sigma = sigma[~np.isnan(sigma)]\n",
    "#     #     sigma = tf.where(tf.math.is_nan(sigma), tf.zeros_like(sigma), sigma)\n",
    "#         sigma_inv = np.diag(sigma**-1)\n",
    "\n",
    "#         psi = np.matmul(np.matmul(A, phi[:, :sigma.shape[0]]), sigma_inv)\n",
    "\n",
    "#         return sigma, psi\n",
    "    \n",
    "     \n",
    "#     def get_K_red_0(self, psi, l):\n",
    "        \n",
    "#         combination = self.beta.shape[1]**self.beta.shape[0]\n",
    "#         K_red_0 = [[None]*combination for i in range(l)]\n",
    "\n",
    "#         for j, mu in enumerate(product(self.beta[0], self.beta[1], self.beta[2])):\n",
    "#             Kc_0 = cp.asarray(pd.read_csv(r\"{path}_Kc0_{b_d}_{b_m}_{b_p}.csv\"\n",
    "#                                 .format(path=self.data_path,b_d=mu[0],b_m=mu[1],b_p=mu[2]), index_col=0).to_numpy())\n",
    "\n",
    "#             print(\"checkpoint {ch}\".format(ch=j))\n",
    "\n",
    "#             for i in range(l):\n",
    "#                 psi_l = psi[:,:i+1]\n",
    "#                 K_red_0[i][j] = cp.matmul(cp.matmul(cp.transpose(psi_l),Kc_0), psi_l)\n",
    "                \n",
    "#         return K_red_0\n",
    "\n",
    "#         self.beta_mu = []\n",
    "#         self.times_mu = []\n",
    "#         self.Uc_mu = []\n",
    "#         self.Fc_mu = []\n",
    "#         for i, mu in enumerate(product(beta[0], beta[1], beta[2])):\n",
    "#             snap_subset = snap[(snap['beta_d'] == mu[0]) & (snap['beta_m'] == mu[1]) & (snap['beta_p'] == mu[2])]\n",
    "#             #status_subset = status[(status['beta_d'] == mu[0]) & (status['beta_m'] == mu[1]) & (status['beta_p'] == mu[2])]\n",
    "            \n",
    "#             self.beta_mu.append(snap_subset.iloc[:, :3].to_numpy())\n",
    "            \n",
    "#             self.times_mu.append(snap_subset.iloc[:, 3].to_numpy())\n",
    "\n",
    "#             self.Uc_mu.append(snap_subset.iloc[:, 4:].to_numpy().T[BC_mask,:])\n",
    "\n",
    "#             self.Fc_mu.append(pd.read_csv(r\"{path}_Fc_{b_d}_{b_m}_{b_p}.csv\"\n",
    "#                                        .format(path=data_path,b_d=mu[0],b_m=mu[1],b_p=mu[2]), index_col=0).to_numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ROM_ML] *",
   "language": "python",
   "name": "conda-env-ROM_ML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
