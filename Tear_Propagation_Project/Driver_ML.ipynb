{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408dc379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.special\n",
    "import math\n",
    "import time\n",
    "from sklearn import model_selection\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import subprocess as sp\n",
    "import os\n",
    "from IPython.display import display, clear_output, Math, Latex\n",
    "\n",
    "import nbimporter\n",
    "from Class_FEA import *\n",
    "from Class_Data_Prep import *\n",
    "import util_func\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "#from keras import callbacks\n",
    "\n",
    "plt.rcParams.update({'font.size':24, 'axes.linewidth':2, 'font.family':'Arial'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8562ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare Abaqus job name, file path to job folder, and user subroutine name \n",
    "job_name = r\"HGO_Tear_Propagation\"\n",
    "job_directory = r\"D:\\Users\\Will\\Tear_Propagation_Project\"\n",
    "umat_name = r\"UMAT_HGO_C_Damage_Alpha\"\n",
    "\n",
    "# Declare work and data directories\n",
    "work_directory = r\"C:\\Users\\swilli9\\STRETCH_Lab_Projects\\Tear_Propagation_Project\"\n",
    "data_directory = r\"{path}\\Data_and_Visualizations\".format(path=work_directory)\n",
    "\n",
    "# Initialize FEA class to retrieve FEA information that is relevant to ROM\n",
    "FE = Abaqus(job_name, job_directory, data_directory, BC_file=r\"{path}\\{job}_BC_mask.csv\"\n",
    "                      .format(path=data_directory,job=job_name))\n",
    "\n",
    "# Define beta parameters\n",
    "beta = np.array([[16.75, 40.78],\n",
    "                 [26.11, 18.85],\n",
    "                 [69.21, 63.20]])\n",
    "\n",
    "# Initialize class for handling POD-BFGS functions and storing ROM data\n",
    "data = FE_Data_Prep(beta, FE)\n",
    "\n",
    "# Initialize storage for time recording\n",
    "offline_times = dict()\n",
    "online_times = dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32fbb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set control for SVD calculation based on whether the calculation has already been run\n",
    "SVD_files = False\n",
    "\n",
    "# Run SVD if needed\n",
    "if not SVD_files:\n",
    "    \n",
    "    # Track run time of svd for offline time summary\n",
    "    start = time.time()\n",
    "    \n",
    "    PhiT, sigma, PsiT  = np.linalg.svd(data.y)\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    # store SVD_time as variable\n",
    "    SVD_time = end-start\n",
    "    \n",
    "    #check cumulative energy for RIC analysis\n",
    "    cumulative_energy = np.cumsum(sigma)/np.sum(sigma)\n",
    "    high_energy_index = np.argwhere(cumulative_energy>=0.9999)[0]\n",
    "    L = int(5*np.ceil(high_energy_index / 5).item())\n",
    "    \n",
    "    # Save necessary subset of singular vectors to dataframes\n",
    "    PsiT = PsiT[:2000, :]\n",
    "    PhiT = PhiT[:, :2000]\n",
    "    \n",
    "    pd.DataFrame(sigma).to_csv(r\"{path}_Sigma.csv\".format(path=data.data_path))\n",
    "    pd.DataFrame(PsiT).to_csv(r\"{path}_Psi.csv\".format(path=data.data_path))\n",
    "    pd.DataFrame(PhiT).to_csv(r\"{path}_Phi.csv\".format(path=data.data_path))\n",
    "\n",
    "# Load SVD results from files if available\n",
    "else:\n",
    "    sigma = pd.read_csv(r\"{path}_Sigma.csv\".format(path=data.data_path), index_col=0).to_numpy().flatten()\n",
    "    PsiT = pd.read_csv(r\"{path}_Psi.csv\".format(path=data.data_path), index_col=0).to_numpy()\n",
    "    PhiT = pd.read_csv(r\"{path}_Phi.csv\".format(path=data.data_path), index_col=0).to_numpy()\n",
    "    \n",
    "    # Compute cumulative energy for RIC analysis\n",
    "    cumulative_energy = np.cumsum(sigma)/np.sum(sigma)\n",
    "    high_energy_index = np.argwhere(cumulative_energy>=0.9999)[0]\n",
    "    L = int(5*np.ceil(high_energy_index / 5).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b177a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot singular value decay and cumulative energy  \n",
    "fig, ax = plt.subplots(1,2, figsize=(15,7.5), dpi=600)\n",
    "\n",
    "ax[0].semilogy(sigma, marker='o', ms=6, linestyle='')\n",
    "\n",
    "ax[1].plot(cumulative_energy, marker='o', ms=6, linestyle='')\n",
    "\n",
    "# Add indicators for 0.9999 RIC threshold\n",
    "ax[0].axvline(x=L, color=\"red\", linestyle=\"--\")\n",
    "ax[1].axvline(x=L, color=\"red\", linestyle=\"--\")\n",
    "ax[0].text(0.11, 0.65, r'$l={r}$'.format(r=L), transform=ax[0].transAxes, size=14)\n",
    "ax[1].text(0.11, 0.65, r'$l={r}$'.format(r=L), transform=ax[1].transAxes, size=14)\n",
    "\n",
    "\n",
    "ax[0].set_ylabel('Singular Values ($\\sigma_{l}$)')\n",
    "ax[1].set_ylabel('Cumulative Energy')\n",
    "for c in [0, 1]:\n",
    "    ax[c].set_xlabel('$l$')\n",
    "    ax[c].set_xlim([-300, 6500])\n",
    "    ax[c].set_xticks(range(0,6001,2000))\n",
    "\n",
    "fig.tight_layout(pad=2.0)\n",
    "\n",
    "# Save figure to EPS file\n",
    "fig.savefig(r\"{path}_sigma_decay_and_cumulative_energy.eps\".format(path=data.data_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd4b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A general class for constructing neural network architecture\n",
    "class NN_Architecture(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "            num_hidden_layers=2, \n",
    "            num_neurons_per_layer=2**7,\n",
    "            input_dim=1,\n",
    "            output_dim=1,\n",
    "            activation=tf.keras.activations.swish,\n",
    "            kernel_initializer=tf.keras.initializers.GlorotNormal(seed=42),\n",
    "            bias_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.5, seed=42),\n",
    "            NN_type = 'DNN',\n",
    "            **kwargs):\n",
    "        \n",
    "        # Intialize superclass with its default parameter signature\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Store hyperparameters\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.output_dim = output_dim     \n",
    "        \n",
    "        # Select DNN architecture\n",
    "        if NN_type == 'DNN':\n",
    "            self.hidden = [tf.keras.layers.Dense(num_neurons_per_layer,\n",
    "                             activation=tf.keras.activations.get(activation),\n",
    "                             kernel_initializer=kernel_initializer, \n",
    "                             bias_initializer=bias_initializer)\n",
    "                             for _ in range(self.num_hidden_layers)]\n",
    "        # Select CNN with concolutional layer and two dense layers\n",
    "        elif NN_type == 'CNN':\n",
    "            conv_list = [tf.keras.layers.Conv1D(32, 3,\n",
    "                             activation=tf.keras.activations.get(activation),\n",
    "                             data_format=\"channels_last\",\n",
    "#                              kernel_initializer=kernel_initializer, \n",
    "#                              bias_initializer=bias_initializer, \n",
    "                             input_shape=(input_dim,1)),\n",
    "                         tf.keras.layers.Flatten()]\n",
    "            \n",
    "            self.hidden = conv_list + [tf.keras.layers.Dense(int((x+1)*(num_neurons_per_layer)/2),\n",
    "                             activation=tf.keras.activations.get(activation),\n",
    "                             kernel_initializer=kernel_initializer, \n",
    "                             bias_initializer=bias_initializer)\n",
    "                             for x in range(self.num_hidden_layers)]\n",
    "            \n",
    "            self.num_hidden_layers = len(self.hidden)\n",
    "        \n",
    "        # Set output layer\n",
    "        self.out = tf.keras.layers.Dense(output_dim)\n",
    "        \n",
    "    \n",
    "    # Mimic functionality of model(x)\n",
    "    def call(self, X):\n",
    "        \n",
    "        #Forward-pass through neural network.\n",
    "        Z = self.hidden[0](X)\n",
    "        for i in range(1, self.num_hidden_layers):\n",
    "            Z = self.hidden[i](Z)\n",
    "        \n",
    "        # return output\n",
    "        return self.out(Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75609982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class for conducting ML training and solving the single domain ML problem\n",
    "class ML_Solver:\n",
    "    def __init__(self, train, test, model_c):\n",
    "        \n",
    "        # save NN model to class object\n",
    "        self.model_c = model_c\n",
    "        \n",
    "        # separate training and testing data for validation to class object\n",
    "        self.x = train[0]\n",
    "        self.y = train[1]\n",
    "        \n",
    "        self.x_val = test[0]\n",
    "        self.y_val = test[1]\n",
    "        \n",
    "        # save maximum y value for normalization to class object\n",
    "        self.y_max = tf.reduce_max(tf.math.abs(self.y), 0, keepdims=True)\n",
    "\n",
    "    \n",
    "    # Compute loss and enforce system and Schwarz boundaries weakly \n",
    "    def get_loss(self, x, y):\n",
    "        \n",
    "        # scale predictions up from normalization\n",
    "        pred = self.model_c(x)*self.y_max\n",
    "\n",
    "        # Compute RMSE-based loss over output tensor\n",
    "        temp_0 = (1/pred.shape[1])*tf.reduce_sum( tf.square(pred - y) , axis=1)\n",
    "        loss = tf.sqrt((1/temp_0.shape[0])*tf.math.reduce_sum(temp_0))   \n",
    "\n",
    "        return loss\n",
    "\n",
    "    # Gets trainable paramter gradients\n",
    "    def get_gradient(self, x, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # This tape is for derivatives with respect to trainable variables\n",
    "            tape.watch(self.model_c.trainable_variables)\n",
    "            \n",
    "            loss = self.get_loss(x, y)\n",
    "\n",
    "        # compute and return gradients for trainable variables\n",
    "        g = tape.gradient(loss, self.model_c.trainable_variables)\n",
    "\n",
    "        return g\n",
    "    \n",
    "    # Primary solving function for ML training\n",
    "    def solve(self, optimizer, numEpochs, batch_size):\n",
    "\n",
    "        # funtion wrapper for graph mode implementation\n",
    "        @tf.function\n",
    "        def train_step(x, y):\n",
    "            # Retrieve loss gradient w.r.t. trainable variables\n",
    "            grad_theta = self.get_gradient(x, y)\n",
    "\n",
    "            # Perform gradient descent step\n",
    "            optimizer.apply_gradients(zip(grad_theta, self.model_c.trainable_variables))\n",
    "\n",
    "        # Split data into training batches\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((self.x, self.y))\n",
    "        train_dataset = train_dataset.shuffle(buffer_size=self.x.shape[0], seed=42).batch(batch_size)\n",
    " \n",
    "        self.training_loss = []\n",
    "        self.validation_loss = []\n",
    "        train_time = 0\n",
    "        # Iterate training\n",
    "        for i in range(numEpochs):\n",
    "            \n",
    "            # Begin tracking training time\n",
    "            start = time.time()\n",
    "            \n",
    "            # Train on each batch\n",
    "            for (x_batch_train, y_batch_train) in train_dataset:\n",
    "                train_step(x_batch_train, y_batch_train)\n",
    "            \n",
    "            #End tracking training time\n",
    "            end = time.time()\n",
    "            \n",
    "            # Add this epoch to training time total\n",
    "            train_time += end-start\n",
    "            \n",
    "            # Compute loss for full dataset to track training progress\n",
    "            loss_train = self.get_loss(self.x, self.y)\n",
    "            loss_val = self.get_loss(self.x_val, self.y_val)\n",
    "            \n",
    "            self.training_loss.append(loss_train)\n",
    "            self.validation_loss.append(loss_val)\n",
    "            \n",
    "            # Display current state of training process to user\n",
    "            clear_output(wait=True)\n",
    "            display(\"Epoch {j}:\".format(j=i+1))\n",
    "            display(\"    Training Loss = {loss}\".format(loss=loss_train))\n",
    "            display(\"    Validation Loss = {loss}\".format(loss=loss_val))\n",
    "        \n",
    "        # return total training time\n",
    "        return train_time\n",
    "            \n",
    "    # Check GPU memory during runs for optimizing batch size    \n",
    "#     def get_gpu_memory(self):\n",
    "#         command = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "#         memory_free_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
    "#         memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "#         return memory_free_values\n",
    "#         base = tf.constant(10.0, shape = y_signs.shape, dtype=tf.float64)\n",
    "#         model_x = tf.math.pow(base,self.model_c(x))*y_signs \n",
    "#         tf.reduce_std(y, 0, keepdims=True) + tf.reduce_mean(y, 0, keepdims=True)\n",
    "#         temp_0 = tf.math.reduce_sum(tf.square( self.model_c(x) - y ), axis=1)\n",
    "#         temp_1 = tf.math.reduce_sum(tf.square( y ), axis=1)\n",
    "#         phi_c = tf.sqrt(tf.math.reduce_sum(temp_0/temp_1))\n",
    "#         phi_c = self.a*tf.math.reduce_mean(tf.square( self.model_c(x) - y ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077d5b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables containing test-case controls\n",
    "train_data_types = [\"POD\", \"full\"]\n",
    "NN_types = [\"DNN\", \"CNN\"]\n",
    "POD_dimensions = [i for i in range(1,6)]+[j for j in range(10,26,5)]+[k for k in range(50,126,25)] + [q for q in range(175,326,50)]\n",
    "\n",
    "# normalize input data and prepare tensor objects for use\n",
    "x_temp = data.x\n",
    "x_normal_feature = np.hstack([np.expand_dims(x_temp[:,i]/np.max(x_temp[:,i]), axis=1) for i in range(x_temp.shape[1])])\n",
    "x = x_normal_feature\n",
    "y = data.y\n",
    "y_tf = tf.constant(y)\n",
    "\n",
    "# set hyperparameters and random seed\n",
    "numEpochs = 2**10\n",
    "n_split = 5\n",
    "timing_reps = 10\n",
    "batch_size = x.shape[0]\n",
    "input_dim = x.shape[1]\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "# Initialize dataframes for output data\n",
    "columns_pareto = [\"NN_type\", \"POD_dim\", \"train_time\", \"test_time\", \"approx_error\", \"approx_rel_error\"]\n",
    "pareto_frame = pd.DataFrame(columns=columns_pareto)\n",
    "\n",
    "columns_loss = [\"NN_type\", \"POD_dim\", \"k-fold\"] + [\"L(epoch_{i})\".format(i=e) for e in range(1,numEpochs+1)]\n",
    "columns_snap_err = [\"NN_type\", \"POD_dim\"] + [\"E(p_{i})\".format(i=s) for s in range(1,x.shape[0]+1)]\n",
    "train_loss_frame = pd.DataFrame(columns=columns_loss)\n",
    "val_loss_frame = pd.DataFrame(columns=columns_loss)\n",
    "snap_err_frame = pd.DataFrame(columns=columns_snap_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c1c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main data gathering loop, iterates over NN types (CNN, DNN) and output data types (POD, full)\n",
    "for dtype, nntype in product(train_data_types, NN_types):\n",
    "    \n",
    "    # shape x arrays based on NN input dimension requirements\n",
    "    if nntype == 'DNN':\n",
    "        x_nn = x \n",
    "    elif nntype == 'CNN':\n",
    "        x_nn = np.reshape(x, (x.shape[0],x.shape[1],1))\n",
    "    \n",
    "    # convert x arrays to tf tensor\n",
    "    x_tf = tf.constant(x_nn)\n",
    "    \n",
    "    # Select controls for POD-ML\n",
    "    if dtype == 'POD':\n",
    "        \n",
    "        # Set optimizer parameters for POD-ML\n",
    "        lr = 1e-3 # tf.keras.optimizers.schedules.PiecewiseConstantDecay([2**6,2**7,2**9],[1e-2,5e-3,1e-3,5e-4])\n",
    "        b1 = 0.95\n",
    "        optimizer = tf.keras.optimizers.Adamax(learning_rate=lr, beta_1=b1, epsilon=1e-7)\n",
    "\n",
    "        # Run POD-ML for all desired basis dimensions\n",
    "        for l in POD_dimensions:\n",
    "            \n",
    "            # Compute true POD coefficients, yPOD, for given basis\n",
    "            Ur = PhiT[:,:l]\n",
    "            Sr = np.diag(sigma[:l])\n",
    "            Vhr = tf.constant(PsiT[:l,:])\n",
    "            yPOD = np.matmul(Ur, Sr)\n",
    "            \n",
    "            # Save true POD displacement field approximation for selected basis dimensions\n",
    "            if (l==1 or l==10 or l==325):\n",
    "                U_svd = np.matmul(yPOD, Vhr)\n",
    "                pd.DataFrame(U_svd).to_csv(r\"{path}_U_svd_{L}.csv\".format(path=data.data_path, L=l))\n",
    "                field_dof_err = np.zeros_like(U_svd)\n",
    "                snap_err = np.zeros((x_nn.shape[0],))\n",
    "                \n",
    "            # Perform k-fold cross-validation procedure\n",
    "            for iModel, (train_index, test_index) in enumerate(model_selection.KFold(n_split, shuffle=True).split(x_nn)):\n",
    "        \n",
    "                # Separate training and testing inputs\n",
    "                if nntype==\"CNN\":\n",
    "                    xtrain, xtest = tf.constant(x_nn[train_index]), tf.constant(x_nn[test_index])\n",
    "                else:\n",
    "                    xtrain, xtest = tf.constant(x_nn[train_index]), tf.constant(x_nn[test_index])\n",
    "                \n",
    "                # Convert POD coefficient training and testing data to tf tensors\n",
    "                ytrain, ytest = tf.constant(yPOD[train_index]), tf.constant(yPOD[test_index])\n",
    "\n",
    "                # Initialize NN model\n",
    "                pod_ml_model = NN_Architecture(num_hidden_layers=2, num_neurons_per_layer=2**6, \n",
    "                                               input_dim = input_dim, output_dim = l, NN_type=nntype)\n",
    "\n",
    "                # Build DNN if necessary\n",
    "                if nntype == 'DNN':\n",
    "                    pod_ml_model.build(input_shape=(None, input_dim))\n",
    "\n",
    "                # Initialize ML solver\n",
    "                pod_ml_solver = ML_Solver( (xtrain, ytrain), (xtest, ytest), pod_ml_model )\n",
    "\n",
    "                # Perform ML training\n",
    "                train_time = pod_ml_solver.solve(optimizer, numEpochs, batch_size)\n",
    "\n",
    "                # Record POD-ML offline time\n",
    "                train_time += SVD_time\n",
    "\n",
    "                # Pre-run prediction in function wrapper\n",
    "                model_predict = tf.function(pod_ml_model) \n",
    "                \n",
    "                # compute POD y_max for online time testing\n",
    "                y_max = tf.math.reduce_max(tf.math.abs(yPOD), 0, keepdims=True)\n",
    "\n",
    "                # Test and record average online prediction time\n",
    "                pred_time = 0\n",
    "                for _ in range(timing_reps):\n",
    "                    start = time.time()\n",
    "\n",
    "                    pred = model_predict(x_tf)*y_max\n",
    "\n",
    "                    end = time.time()\n",
    "\n",
    "                    pred_time += end-start\n",
    "                pred_time = pred_time/timing_reps\n",
    "                \n",
    "                # Compute POD-ML prediction of displacement field\n",
    "                U_pred = np.matmul(pred, Vhr)\n",
    "\n",
    "                # compute relative and non-relative weighted RMSE between POD-ML and FE displacement results\n",
    "                approx_square_norm = (1/U_pred.shape[1])*np.sum(np.square(U_pred-y_tf), axis=1)\n",
    "                approx_rel_square_norm = approx_square_norm/np.sum(np.square(y_tf), axis=1)\n",
    "                \n",
    "                approx_rmse = np.sqrt( (1/len(approx_square_norm)) * np.sum(approx_square_norm) ).item()\n",
    "                approx_rmse_rel = np.sqrt( (1/len(approx_rel_square_norm)) * np.sum(approx_rel_square_norm) ).item()\n",
    "                \n",
    "                # Save relevant results to dataframe\n",
    "                pareto_frame.loc[len(pareto_frame)] = [nntype, l, train_time, pred_time, approx_rmse, approx_rmse_rel]\n",
    "\n",
    "                # Sum snapshot and nodal errors and \n",
    "                # save training and validation loss results for selected basis dimensions\n",
    "                if (l==1 or l==10 or l==325):\n",
    "                    field_dof_err += np.sqrt(np.square(U_pred-U_svd))\n",
    "                    snap_err += np.sqrt(approx_square_norm).flatten()\n",
    "                    \n",
    "                    loss_id = np.array([nntype, l, iModel+1])\n",
    "                    training_loss_row = np.hstack( (loss_id, np.array(pod_ml_solver.training_loss)) )\n",
    "                    validation_loss_row = np.hstack( (loss_id, np.array(pod_ml_solver.validation_loss)) )\n",
    "                    \n",
    "                    train_loss_frame.loc[len(train_loss_frame)] = training_loss_row\n",
    "                    val_loss_frame.loc[len(val_loss_frame)] = validation_loss_row\n",
    "\n",
    "            # Save snapshot-specific and nodal RMSE for selected basis dimensions\n",
    "            if (l==1 or l==10 or l==325):\n",
    "                field_dof_err = field_dof_err/n_split\n",
    "                pd.DataFrame(field_dof_err).to_csv(\n",
    "                                                    r\"{path}_field_dof_error_pod_{L}_{nn}.csv\".format(path=data.data_path, L=l, nn=nntype)\n",
    "                                                    )\n",
    "                \n",
    "                snap_err = snap_err/n_split\n",
    "                err_id = np.array([nntype, l])\n",
    "                snap_err_row = np.hstack((err_id, snap_err))\n",
    "                \n",
    "                snap_err_frame.loc[len(snap_err_frame)] = snap_err_row\n",
    "\n",
    "    # Select controls for full-field output ML\n",
    "    else:\n",
    "        \n",
    "        # Set optimizer parameter for full-field ML\n",
    "        l=0\n",
    "        lr = 5e-4 # tf.keras.optimizers.schedules.PiecewiseConstantDecay([2**6,2**7,2**9],[1e-2,5e-3,1e-3,5e-4])\n",
    "        b1 = 0.9\n",
    "        optimizer = tf.keras.optimizers.Adamax(learning_rate=lr, beta_1=b1, epsilon=1e-7)\n",
    "\n",
    "        # convert training data array to tf tensor and initialize results data storage\n",
    "        y_tf = tf.constant(y)\n",
    "        field_dof_err = np.zeros_like(y)\n",
    "        snap_err = np.zeros((x_nn.shape[0],))\n",
    "\n",
    "        # Perform k-fold cross-validation procedure\n",
    "        for iModel, (train_index, test_index) in enumerate(model_selection.KFold(n_split, shuffle=True).split(x_nn)):\n",
    "\n",
    "            # Separate training and testing data and store in tf tensors\n",
    "            xtrain, xtest = tf.constant(x_nn[train_index]), tf.constant(x_nn[test_index])\n",
    "\n",
    "            ytrain, ytest = tf.constant(y[train_index]), tf.constant(y[test_index])\n",
    "\n",
    "            # Intialize ML model\n",
    "            ml_model = NN_Architecture(num_hidden_layers=2, num_neurons_per_layer=2**6, \n",
    "                                           input_dim = input_dim, output_dim = y.shape[1], NN_type=nntype)\n",
    "\n",
    "            # Build DNN if necessary\n",
    "            if nntype == 'DNN':\n",
    "                ml_model.build(input_shape=(None, input_dim))\n",
    "\n",
    "            # Initialize ML solver\n",
    "            ml_solver = ML_Solver( (xtrain, ytrain), (xtest, ytest), ml_model )\n",
    "\n",
    "            # Train ML model\n",
    "            train_time = ml_solver.solve(optimizer, numEpochs, batch_size)\n",
    "\n",
    "            # Pre-run prediction in function wrapper\n",
    "            model_predict = tf.function(ml_model) \n",
    "            \n",
    "            # Compute y_max for online time testing\n",
    "            y_max = tf.math.reduce_max(tf.math.abs(y_tf), 0, keepdims=True)\n",
    "\n",
    "            # Test and record average online prediction time\n",
    "            pred_time = 0\n",
    "            for _ in range(timing_reps):\n",
    "                start = time.time()\n",
    "\n",
    "                pred = model_predict(x_tf)*y_max\n",
    "\n",
    "                end = time.time()\n",
    "\n",
    "                pred_time += end-start\n",
    "            pred_time = pred_time/timing_reps\n",
    "            \n",
    "            # compute relative and non-relative weighted RMSE between POD-ML and FE displacement results\n",
    "            approx_square_norm = (1/U_pred.shape[1])*np.sum(np.square(U_pred-y_tf), axis=1)\n",
    "            approx_rel_square_norm = approx_square_norm/np.sum(np.square(y_tf), axis=1)\n",
    "            \n",
    "            approx_rmse = np.sqrt( (1/len(approx_square_norm)) * np.sum(approx_square_norm) ).item()\n",
    "            approx_rmse_rel = np.sqrt( (1/len(approx_rel_square_norm)) * np.sum(approx_rel_square_norm) ).item()\n",
    "\n",
    "            # save relevant results to dataframe\n",
    "            pareto_frame.loc[len(pareto_frame)] = [nntype, l, train_time, pred_time, approx_rmse, approx_rmse_rel]\n",
    "\n",
    "            # Sum snapshot specific and nodal error\n",
    "            field_dof_err += np.sqrt(np.square(pred-y_tf))\n",
    "            snap_err += np.sqrt(approx_square_norm).flatten()\n",
    "\n",
    "            # Save training and validation loss results\n",
    "            loss_id = np.array([nntype, l, iModel+1])\n",
    "            training_loss_row = np.hstack( (loss_id, np.array(ml_solver.training_loss)) )\n",
    "            validation_loss_row = np.hstack( (loss_id, np.array(ml_solver.validation_loss)) )\n",
    "\n",
    "            train_loss_frame.loc[len(train_loss_frame)] = training_loss_row\n",
    "            val_loss_frame.loc[len(val_loss_frame)] = validation_loss_row\n",
    "\n",
    "        # Save snapshot and nodal RMSE results\n",
    "        field_dof_err = field_dof_err/n_split\n",
    "        pd.DataFrame(field_dof_err).to_csv(\n",
    "                                            r\"{path}_field_dof_error_full_{nn}.csv\".format(path=data.data_path, nn=nntype)\n",
    "                                            )\n",
    "\n",
    "        snap_err = snap_err/n_split\n",
    "        err_id = np.array([nntype, l])\n",
    "        snap_err_row = np.hstack((err_id, snap_err))\n",
    "\n",
    "        snap_err_frame.loc[len(snap_err_frame)] = snap_err_row\n",
    "\n",
    "display(\"ANALYSIS COMPLETE.\")\n",
    "\n",
    "# Save all results dataframes to CSV for later use\n",
    "pareto_frame.to_csv(r\"{path}_pareto_frame.csv\".format(path=data.data_path))\n",
    "train_loss_frame.to_csv(r\"{path}_train_loss_frame.csv\".format(path=data.data_path))\n",
    "val_loss_frame.to_csv(r\"{path}_val_loss_frame.csv\".format(path=data.data_path))\n",
    "snap_err_frame.to_csv(r\"{path}_snap_err_frame.csv\".format(path=data.data_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6697d9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59643cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute true POD approximation error as benchmark for POD-ML\n",
    "svd_err = []\n",
    "for l in POD_dimensions:\n",
    "    Ur = PhiT[:,:l]\n",
    "    Sr = np.diag(sigma[:l])\n",
    "    Vhr = tf.constant(PsiT[:l,:])\n",
    "\n",
    "    yPOD = np.matmul(Ur, Sr)\n",
    "    \n",
    "    U_svd = np.matmul(yPOD, Vhr)\n",
    "    \n",
    "    temp = (1/U_svd.shape[1])*np.sum(np.square(U_svd-y_tf), axis=1)\n",
    "    \n",
    "    svd_err.append(np.sqrt( (1/len(temp)) * np.sum(temp) ).item())\n",
    "    \n",
    "pd.DataFrame(svd_err).to_csv(r\"{path}_SVD_norm_error.csv\".format(path=data.data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1197ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load results dataframes\n",
    "pareto_frame = pd.read_csv(r\"{path}_pareto_frame.csv\".format(path=data.data_path), index_col=0)\n",
    "train_loss_frame = pd.read_csv(r\"{path}_train_loss_frame.csv\".format(path=data.data_path), index_col=0)\n",
    "val_loss_frame = pd.read_csv(r\"{path}_val_loss_frame.csv\".format(path=data.data_path), index_col=0)\n",
    "snap_err_frame = pd.read_csv(r\"{path}_snap_err_frame.csv\".format(path=data.data_path), index_col=0)\n",
    "svd_err=pd.read_csv(r\"{path}_SVD_norm_error.csv\".format(path=data.data_path), index_col=0).values.flatten()\n",
    "\n",
    "# Plot POD-ML versus true POD results for RMSE with respect to FE versus basis dimension, l\n",
    "\n",
    "pareto_results = pareto_frame.groupby([\"NN_type\",\"POD_dim\"])[[\"train_time\", \"test_time\", \"approx_error\"]].mean()\n",
    "\n",
    "CNN_results = pareto_results.loc[\"CNN\"]\n",
    "DNN_results = pareto_results.loc[\"DNN\"]\n",
    "\n",
    "marker = [\"o\", \"^\", \"s\", \"v\", \"P\", \"*\", \"D\", \"p\"]\n",
    "marker_size = 10\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(15,7.5), dpi=600)\n",
    "    \n",
    "ax.semilogy(DNN_results.index[1:], DNN_results[\"approx_error\"][1:], marker=marker[0], ms=marker_size, linestyle='', label=\"DNN POD-ML\")\n",
    "ax.semilogy(CNN_results.index[1:], CNN_results[\"approx_error\"][1:], marker=marker[1], ms=marker_size, linestyle='', label=\"CNN POD-ML\")\n",
    "ax.semilogy(DNN_results.index[1:], svd_err, marker=marker[2], ms=marker_size, linestyle='', label=\"True POD\")\n",
    "\n",
    "ax.set_ylabel('$\\mathcal{E}_{POD-ML}$')\n",
    "ax.set_xlabel('$l$')\n",
    "ax.set_xlim([-10, 335])\n",
    "ax.set_ylim([1e-6, 5e-1])\n",
    "ax.set_xticks([1]+[_ for _ in range(25,326,25)])\n",
    "ax.legend(loc='best', fontsize=18, ncol=1, markerscale = 1, columnspacing=0.8, \n",
    "             handletextpad=0.1, borderpad=0.3, framealpha=1, edgecolor='black')\n",
    "\n",
    "fig.tight_layout(pad=2.0)\n",
    "fig.savefig(r\"{path}_pod_ml_approx_error.eps\".format(path=data.data_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e18f221",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PLot ML and POD-ML results for RMSE versus training time and prediction time\n",
    "\n",
    "marker = [\"o\", \"^\", \"s\", \"v\", \"P\", \"*\", \"D\", \"p\"]\n",
    "fig, ax = plt.subplots(1,2, figsize=(15,7.5), dpi=600)\n",
    "\n",
    "marker_size = 10\n",
    "\n",
    "ax[0].loglog(CNN_results[\"train_time\"][0], CNN_results[\"approx_error\"][0], marker=marker[0], ms=marker_size, linestyle='', label=\"CNN\")\n",
    "ax[0].loglog(DNN_results[\"train_time\"][0], DNN_results[\"approx_error\"][0], marker=marker[1], ms=marker_size, linestyle='', label=\"DNN\")\n",
    "ax[0].loglog(CNN_results[\"train_time\"][1:], CNN_results[\"approx_error\"][1:], marker=marker[2], ms=marker_size, linestyle='', label=\"CNN POD-ML\")\n",
    "ax[0].loglog(DNN_results[\"train_time\"][1:], DNN_results[\"approx_error\"][1:], marker=marker[3], ms=marker_size, linestyle='', label=\"DNN POD-ML\")\n",
    "ax[0].set_xlim([10, 1100])\n",
    "\n",
    "\n",
    "ax[1].semilogy(CNN_results[\"test_time\"][0], CNN_results[\"approx_error\"][0], marker=marker[0], ms=marker_size, linestyle='', label=\"CNN\", zorder=15)\n",
    "ax[1].semilogy(DNN_results[\"test_time\"][0], DNN_results[\"approx_error\"][0], marker=marker[1], ms=marker_size, linestyle='', label=\"DNN\", zorder=10)\n",
    "ax[1].semilogy(CNN_results[\"test_time\"][1:], CNN_results[\"approx_error\"][1:], marker=marker[2], ms=marker_size, linestyle='', label=\"CNN POD-ML\", zorder=5)\n",
    "ax[1].semilogy(DNN_results[\"test_time\"][1:], DNN_results[\"approx_error\"][1:], marker=marker[3], ms=marker_size, linestyle='', label=\"DNN POD-ML\", zorder=0)\n",
    "ax[1].set_xlim([0.002, 0.008])\n",
    "\n",
    "ax[0].set_xlabel('Training Time (s)')\n",
    "ax[1].set_xlabel('Prediction Time (s)')\n",
    "\n",
    "fig.supylabel('$\\mathcal{E}_{ML}$ / $\\mathcal{E}_{POD-ML}$', x=0.05, y=0.55)\n",
    "\n",
    "for c in range(len(ax)):\n",
    "    ax[c].set_ylim([0.008, 1])\n",
    "    ax[c].legend(loc='best', fontsize=18, ncol=1, markerscale = 1, columnspacing=0.8, \n",
    "             handletextpad=0.1, borderpad=0.3, framealpha=1, edgecolor='black')\n",
    "\n",
    "fig.tight_layout(pad=2.0)\n",
    "fig.savefig(r\"{path}_pareto_train_predict.eps\".format(path=data.data_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5268c980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c47c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ML and POD-ML results for RMSE of individual snapshots across parameter cases and luminal pressures\n",
    "\n",
    "xplot = np.array(data.x[:,3]).flatten()\n",
    "start_ind = np.sort(np.argsort(xplot)[:8])\n",
    "stop_ind = np.hstack([start_ind[1:], len(xplot)])\n",
    "marker = [\"o\", \"^\", \"s\", \"v\", \"P\", \"*\", \"D\", \"p\"]\n",
    "color = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#FFE933', '#e377c2', '#33E6FF']\n",
    "\n",
    "fig, ax = plt.subplots(4,2, figsize=(15,20), dpi=600)\n",
    "# ax=ax.flatten()\n",
    "df_temp = snap_err_frame.reindex([6, 0, 1, 8, 2, 7, 3, 9, 4, 5])\n",
    "\n",
    "r=0\n",
    "c=0\n",
    "for f in range(len(snap_err_frame)):\n",
    "    row = df_temp.iloc[f]\n",
    "    E = np.array(row.iloc[2:].values, dtype=float) \n",
    "    \n",
    "    if row[\"POD_dim\"] == 25:\n",
    "        continue\n",
    "        \n",
    "    if row[\"POD_dim\"] == 0:\n",
    "        title = \"{nn}\".format(nn=row[\"NN_type\"])\n",
    "    else:\n",
    "        title = \"{nn} POD $l={L}$\".format(nn=row[\"NN_type\"], L=row[\"POD_dim\"])\n",
    "        l=int(row.iloc[1])\n",
    "    \n",
    "    if (row[\"NN_type\"] == \"CNN\" and row[\"POD_dim\"]==0):\n",
    "        r=0\n",
    "        c=1\n",
    "    \n",
    "    for i in range(len(stop_ind)):\n",
    "        label = \"$\\mu_{b}$\".format(b=i+1)    \n",
    "        ax[r,c].semilogy(xplot[start_ind[i]:stop_ind[i]], E[start_ind[i]:stop_ind[i]], color=color[i], marker=marker[i], ms=6, linestyle='', label=label)\n",
    "\n",
    "    ax[r,c].set_title(title)\n",
    "    ax[r,c].set_xlim([-2, 53])\n",
    "    ax[r,c].set_xticks(np.array(range(0,51,5)))\n",
    "    ax[r,c].set_ylim([float(f'1e-3'), float(f'1e0')])\n",
    "    \n",
    "    if r+c == 0:\n",
    "        ax[r,c].legend(loc='best', fontsize=20, ncol=4, markerscale = 1.5, columnspacing=0.8, \n",
    "             handletextpad=0.1, borderpad=0.3, framealpha=1, edgecolor='black')\n",
    "        ax[r,c].set_ylabel(\"$\\mathcal{E}^{(i)}_{ML}$\")\n",
    "    elif c==0:\n",
    "        ax[r,c].set_ylabel(\"$\\mathcal{E}^{(i)}_{POD-ML}$\")\n",
    "    r+=1\n",
    "\n",
    "fig.supxlabel('Pressure (kPa)', x=0.55, y=0.05)\n",
    "fig.tight_layout(pad=2.0)\n",
    "fig.savefig(r\"{path}_err_vs_pressure.eps\".format(path=data.data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f21cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block plots a subset of the above block's plots to abridge results for presentations\n",
    "\n",
    "xplot = np.array(data.x[:,3]).flatten()\n",
    "start_ind = np.sort(np.argsort(xplot)[:8])\n",
    "stop_ind = np.hstack([start_ind[1:], len(xplot)])\n",
    "marker = [\"o\", \"^\", \"s\", \"v\", \"P\", \"*\", \"D\", \"p\"]\n",
    "color = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#FFE933', '#e377c2', '#33E6FF']\n",
    "\n",
    "fig, ax = plt.subplots(2,2, figsize=(15,11), dpi=600)\n",
    "df_temp = snap_err_frame.reindex([6, 0, 1, 8, 2, 7, 3, 9, 4, 5])\n",
    "\n",
    "r=0\n",
    "c=0\n",
    "for f in range(len(snap_err_frame)):\n",
    "    row = df_temp.iloc[f]\n",
    "    E = np.array(row.iloc[2:].values, dtype=float)\n",
    "    \n",
    "    if (row[\"POD_dim\"] == 1 or row[\"POD_dim\"] > 10):\n",
    "        continue\n",
    "        \n",
    "    if row[\"POD_dim\"] == 0:\n",
    "        title = \"{nn}\".format(nn=row[\"NN_type\"])\n",
    "    else:\n",
    "        title = \"{nn} POD $l={L}$\".format(nn=row[\"NN_type\"], L=row[\"POD_dim\"])\n",
    "        l=int(row.iloc[1])\n",
    "    \n",
    "    if (row[\"NN_type\"] == \"CNN\" and row[\"POD_dim\"]==0):\n",
    "        r=0\n",
    "        c=1\n",
    "    \n",
    "    for i in range(len(stop_ind)):\n",
    "        label = \"$\\mu_{b}$\".format(b=i+1)    \n",
    "        ax[r,c].semilogy(xplot[start_ind[i]:stop_ind[i]], E[start_ind[i]:stop_ind[i]], color=color[i], marker=marker[i], ms=6, linestyle='', label=label)\n",
    "\n",
    "    ax[r,c].set_title(title)\n",
    "    ax[r,c].set_xlim([-2, 53])\n",
    "    ax[r,c].set_xticks(np.array(range(0,51,5)))\n",
    "    ax[r,c].set_ylim([float(f'1e-3'), float(f'1e0')])\n",
    "    \n",
    "    if r+c == 0:\n",
    "        ax[r,c].legend(loc='best', fontsize=20, ncol=4, markerscale = 1.5, columnspacing=0.8, \n",
    "             handletextpad=0.1, borderpad=0.3, framealpha=1, edgecolor='black')\n",
    "        ax[r,c].set_ylabel(\"$\\mathcal{E}^{(i)}_{ML}$\")\n",
    "    elif c==0:\n",
    "        ax[r,c].set_ylabel(\"$\\mathcal{E}^{(i)}_{POD-ML}$\")\n",
    "    r+=1\n",
    "\n",
    "fig.supxlabel('Pressure (kPa)', x=0.53, y=0.1)\n",
    "fig.tight_layout(pad=2.0)\n",
    "fig.savefig(r\"{path}_err_vs_pressure_sub_figure.eps\".format(path=data.data_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedeb829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss for cross-validation procedures from ML and selected POD-ML models\n",
    "\n",
    "train_loss = train_loss_frame.groupby([\"NN_type\", \"POD_dim\", \"k-fold\"]).sum()\n",
    "val_loss = val_loss_frame.groupby([\"NN_type\", \"POD_dim\", \"k-fold\"]).sum()\n",
    "\n",
    "dim_list = [\"0\",\"1\",\"10\",\"325\"]\n",
    "marker = [\"o\", \"^\", \"s\", \"v\", \"P\", \"*\", \"D\", \"p\"]\n",
    "color = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#FFE933', '#e377c2', '#33E6FF']\n",
    "\n",
    "fig_dnn, ax_dnn = plt.subplots(4,2, figsize=(15,20), dpi=600)\n",
    "# ax_dnn=ax_dnn.flatten()\n",
    "\n",
    "fig_cnn, ax_cnn = plt.subplots(4,2, figsize=(15,20), dpi=600)\n",
    "# ax_cnn=ax_cnn.flatten()\n",
    "\n",
    "ylims = [[1e-2, 2e0], [1e-1, 1e3], [1e-1, 1e2], [1e-1, 2e1]]\n",
    "titles = [\"ML\", \"POD-ML $l=1$\", \"POD-ML $l=25$\", \"POD-ML $l=325$\"]\n",
    "\n",
    "d=0\n",
    "c=0\n",
    "for nn, dim in product(NN_types, dim_list):\n",
    "    temp0 = train_loss.loc[nn, dim]\n",
    "    temp1 = val_loss.loc[nn, dim]\n",
    "    \n",
    "    if nn == \"DNN\":\n",
    "        ax = ax_dnn\n",
    "\n",
    "        ax[d, 0].set_ylabel('Training Loss')\n",
    "        ax[d, 1].set_ylabel('Validation Loss')\n",
    "        ax[d, 0].set_xlim([-100, 1124])\n",
    "        ax[d, 1].set_xlim([-100, 1124])\n",
    "        \n",
    "        ax[d, 0].set_title(titles[d])\n",
    "        ax[d, 1].set_title(titles[d])\n",
    "\n",
    "        ax[d, 0].set_ylim(ylims[d])\n",
    "        ax[d, 1].set_ylim(ylims[d])\n",
    "        for i in range(1,6):\n",
    "            \n",
    "            ax[d, 0].semilogy([q for q in range(1,1025)], temp0.loc[i].to_numpy(dtype=float), color=color[i], marker=marker[i-1], ms=5, linestyle='', label=\"Model {i}\".format(i=i))\n",
    "            ax[d, 1].semilogy([q for q in range(1,1025)], temp0.loc[i].to_numpy(dtype=float), color=color[i], marker=marker[i-1], ms=5, linestyle='', label=\"Model {i}\".format(i=i))\n",
    "\n",
    "        if d == 0:\n",
    "            ax[d, 0].legend(loc='best', fontsize=20, ncol=2, markerscale = 1.5, columnspacing=0.8, \n",
    "                 handletextpad=0.1, borderpad=0.3, framealpha=1, edgecolor='black')\n",
    "\n",
    "        d += 1\n",
    "    elif nn == \"CNN\":\n",
    "        ax = ax_cnn\n",
    "        \n",
    "        ax[c, 0].set_ylabel('Training Loss')\n",
    "        ax[c, 1].set_ylabel('Validation Loss')\n",
    "        ax[c, 0].set_xlim([-100, 1124])\n",
    "        ax[c, 1].set_xlim([-100, 1124])\n",
    "        \n",
    "        ax[c, 0].set_title(titles[c])\n",
    "        ax[c, 1].set_title(titles[c])\n",
    "            \n",
    "        ax[c, 0].set_ylim(ylims[c])\n",
    "        ax[c, 1].set_ylim(ylims[c])\n",
    "        for i in range(1,6):\n",
    "            ax[c, 0].semilogy([q for q in range(1,1025)], temp0.loc[i].to_numpy(dtype=float), marker=marker[i-1], ms=5, linestyle='', label=\"Model {i}\".format(i=i))\n",
    "            ax[c, 1].semilogy([q for q in range(1,1025)], temp0.loc[i].to_numpy(dtype=float), marker=marker[i-1], ms=5, linestyle='', label=\"Model {i}\".format(i=i))\n",
    "\n",
    "        if c == 0:\n",
    "            ax[c, 0].legend(loc='best', fontsize=20, ncol=2, markerscale = 1.5, columnspacing=0.8, \n",
    "                 handletextpad=0.1, borderpad=0.3, framealpha=1, edgecolor='black')\n",
    "        c += 1\n",
    "\n",
    "        \n",
    "fig_dnn.supxlabel('Epoch', x=0.53, y=0.05)\n",
    "fig_cnn.supxlabel('Epoch', x=0.53, y=0.05)\n",
    "\n",
    "fig_dnn.tight_layout(pad=2.0)\n",
    "fig_cnn.tight_layout(pad=2.0)\n",
    "\n",
    "fig_dnn.savefig(r\"{path}_loss_dnn.eps\".format(path=data.data_path))\n",
    "fig_cnn.savefig(r\"{path}_loss_cnn.eps\".format(path=data.data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c49b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afe1273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot tear length versus pressure across all parameter cases \n",
    "# and output averages and standard deviations of final tear length and tear length at initial propagation\n",
    "from scipy import stats\n",
    "fig, ax = plt.subplots(1,1, figsize=(15,7.5), dpi=600)\n",
    "\n",
    "xplot = np.array(data.x[:,3]).flatten()\n",
    "start_ind = np.sort(np.argsort(xplot)[:8])\n",
    "stop_ind = np.hstack([start_ind[1:], len(xplot)])\n",
    "\n",
    "marker = [\"o\", \"^\", \"s\", \"v\", \"P\", \"*\", \"D\", \"p\"]\n",
    "color = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#FFE933', '#e377c2', '#33E6FF']\n",
    "\n",
    "avg_a = []\n",
    "tp_init=[]\n",
    "for i in range(len(stop_ind)):\n",
    "    ax.plot(data.x[start_ind[i]:stop_ind[i],3], \n",
    "               data.x[start_ind[i]:stop_ind[i],4]+data.x[start_ind[i]:stop_ind[i],5], color=color[i], \n",
    "               marker=marker[i], ms=6, linestyle='', label=\"$\\mu_{j}$\".format(j=i+1))\n",
    "    avg_a.append(data.x[start_ind[i]:stop_ind[i],4][-1]+data.x[start_ind[i]:stop_ind[i],5][-1])\n",
    "    tp_init.append(stats.mode(data.x[start_ind[i]:stop_ind[i],4])[0]+stats.mode(data.x[start_ind[i]:stop_ind[i],4])[0])\n",
    "    \n",
    "avg_a = np.array(avg_a)\n",
    "tp_init = np.array(tp_init)\n",
    "\n",
    "print(np.mean(avg_a), np.std(avg_a))\n",
    "print(np.mean(tp_init), np.std(tp_init))\n",
    "ax.set_ylabel('Tear Length (mm)')\n",
    "ax.set_xlabel('Pressure (kPa)')\n",
    "ax.set_xlim([-2, 53])\n",
    "ax.set_xticks([_ for _ in range(0,51,5)])\n",
    "ax.legend(loc='best', fontsize=18, ncol=2, markerscale = 1.5, columnspacing=0.8, \n",
    "             handletextpad=0.1, borderpad=0.3, framealpha=1, edgecolor='black')\n",
    "fig.tight_layout(pad=2.0)\n",
    "\n",
    "fig.savefig(r\"{path}_tear_length_vs_pressure.eps\".format(path=data.data_path))\n",
    "\n",
    "\n",
    "# Plot stress concentration factors versus pressure for all parameter cases\n",
    "# and output final stress concentration factors at proximal and distal heads fo the tear\n",
    "fig, ax = plt.subplots(1,2, figsize=(15,7.5), dpi=600)\n",
    "\n",
    "avg_d = []\n",
    "avg_p = []\n",
    "for i in range(len(stop_ind)):\n",
    "    ax[0].plot(data.x[start_ind[i]:stop_ind[i],3], data.x[start_ind[i]:stop_ind[i],11], color=color[i], marker=marker[i], ms=6, linestyle='', label=\"$\\mu_{j}$\".format(j=i+1))\n",
    "    ax[1].plot(data.x[start_ind[i]:stop_ind[i],3], data.x[start_ind[i]:stop_ind[i],12], color=color[i], marker=marker[i], ms=6, linestyle='', label=\"$\\mu_{j}$\".format(j=i+1))\n",
    "    avg_d.append(data.x[start_ind[i]:stop_ind[i],11][-1])\n",
    "    avg_p.append(data.x[start_ind[i]:stop_ind[i],12][-1])\n",
    "\n",
    "avg_d = np.array(avg_d)\n",
    "avg_p = np.array(avg_p)\n",
    "print(np.mean(avg_d), np.std(avg_d))\n",
    "print(np.mean(avg_p), np.std(avg_p))\n",
    "    \n",
    "ax[0].set_ylabel('$SCF_d$')\n",
    "ax[1].set_ylabel('$SCF_p$') \n",
    "    \n",
    "for c in [0,1]:\n",
    "\n",
    "    ax[c].set_xlim([-2, 53])\n",
    "    ax[c].set_xticks([_ for _ in range(0,51,5)])\n",
    "    ax[c].set_ylim([2, 12])\n",
    "\n",
    "ax[0].legend(loc='best', fontsize=18, ncol=2, markerscale = 1.5, columnspacing=0.8, \n",
    "             handletextpad=0.1, borderpad=0.3, framealpha=1, edgecolor='black')\n",
    "\n",
    "fig.supxlabel(\"Pressure (kPa)\", y=0.15, x=0.52)\n",
    "fig.tight_layout(pad=2.0)\n",
    "fig.savefig(r\"{path}_SCF_vs_pressure.eps\".format(path=data.data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539a0366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ROM_ML] *",
   "language": "python",
   "name": "conda-env-ROM_ML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
