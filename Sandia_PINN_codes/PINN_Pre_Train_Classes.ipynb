{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c7bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nbimporter\n",
    "from PDE_Classes import *\n",
    "\n",
    "# A general class for constructing neural network architecture\n",
    "class PINN_Architecture(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "            num_hidden_layers=3, \n",
    "            num_neurons_per_layer=20,\n",
    "            output_dim=1,\n",
    "            activation=tf.keras.activations.swish,\n",
    "            kernel_initializer='glorot_normal',\n",
    "            bias_initializer='zeros',\n",
    "            **kwargs):\n",
    "        \n",
    "        # Intialize superclass with its default parameter signature\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Store hyperparameters\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.output_dim = output_dim     \n",
    "        \n",
    "        # Define NN architecture\n",
    "        self.hidden = [tf.keras.layers.Dense(num_neurons_per_layer,\n",
    "                             activation=tf.keras.activations.get(activation),\n",
    "                             kernel_initializer=kernel_initializer, \n",
    "                             bias_initializer=bias_initializer)\n",
    "                           for _ in range(self.num_hidden_layers)]\n",
    "        self.out = tf.keras.layers.Dense(output_dim)\n",
    "        \n",
    "    \n",
    "    # Mimic functionality of model(x)\n",
    "    def call(self, X):\n",
    "        #Forward-pass through neural network.\n",
    "        Z = self.hidden[0](X)\n",
    "        for i in range(1, self.num_hidden_layers):\n",
    "            Z = self.hidden[i](Z)\n",
    "        return self.out(Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fc60d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN_Schwarz_Steady():\n",
    "    def __init__(self, pde, model_r, model_i, X_r, X_b, alpha, strong, snap):\n",
    "        \n",
    "        # Store PDE\n",
    "        self.pde = pde\n",
    "        \n",
    "        # Store models \n",
    "        self.model_r = model_r\n",
    "        self.model_i = model_i\n",
    "\n",
    "        # Store internal collocation points\n",
    "        self.x = X_r\n",
    "\n",
    "        # Store boundary points\n",
    "        self.xb = X_b\n",
    "        \n",
    "        self.strong = strong\n",
    "\n",
    "        # Store snapshot points if applicable\n",
    "        if snap:\n",
    "            self.npxs = np.linspace(float(self.xb[0][0][0]), float(self.xb[1][0][0]), num=snap, \n",
    "                                endpoint=False)[1:]\n",
    "            \n",
    "            self.xs = tf.concat([tf.constant(self.npxs, shape=(snap-1, 1), dtype='float64'), \n",
    "                                tf.constant(self.xb[0][0][1], shape=(snap-1, 1), dtype='float64')], 1)\n",
    "        self.snap = snap\n",
    "        \n",
    "        # Store loss scaling coefficient\n",
    "        self.a = alpha\n",
    "        \n",
    "        self.loss = 0\n",
    "        self.err = 0\n",
    "\n",
    "    \n",
    "    def BC_enforce(self, x):\n",
    "        y = tf.reshape(x[:,0], shape=(x.shape[0], 1))\n",
    "        return tf.math.tanh( (1-y) )*tf.math.tanh( y )\n",
    "\n",
    "    \n",
    "    @tf.function\n",
    "    def get_residual(self, x):\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Watch variable x during this GradientTape\n",
    "            tape.watch(x)\n",
    "\n",
    "            # Compute current values u(x) with strongly enforced BCs\n",
    "            if self.strong:\n",
    "                u = self.BC_enforce(x)*self.model_r(x)\n",
    "            else:\n",
    "                u = self.model_r(x)\n",
    "\n",
    "            # Store first derivative\n",
    "            u_x = tape.gradient(u, x)\n",
    "            \n",
    "        # Store second derivative \n",
    "        u_xx = tape.gradient(u_x, x)\n",
    "\n",
    "        del tape\n",
    "\n",
    "        return self.pde.f_r(u_x, u_xx)\n",
    "\n",
    "    \n",
    "    def loss_strong(self, x):\n",
    "\n",
    "        # Compute phi_r\n",
    "        r = self.get_residual(x)\n",
    "        phi_r = self.a * tf.reduce_mean(tf.square(r))\n",
    "\n",
    "        # Initialize loss with residual loss function\n",
    "        loss = phi_r\n",
    "\n",
    "        phi_i = 0\n",
    "        for i,model in enumerate(self.model_i):\n",
    "            if not model:\n",
    "                continue\n",
    "\n",
    "            b = self.xb[i]\n",
    "\n",
    "            # Calculate interface loss for current model if applicable\n",
    "            u_pred1 = self.BC_enforce(b)*self.model_r(b)\n",
    "            if isinstance(model[0], FD_1D_Steady):\n",
    "                u_pred2 = model[0](b)\n",
    "            else:\n",
    "                u_pred2 = self.BC_enforce(b)*model[0](b)   \n",
    "            phi_i += (1 - self.a) * tf.reduce_mean(tf.square(u_pred1 - u_pred2))\n",
    "\n",
    "        phi_s = 0\n",
    "        if self.snap:\n",
    "            # calculate snapshot data loss\n",
    "            phi_s = (1 - self.a) * tf.reduce_mean(tf.square( self.BC_enforce(self.xs)*self.model_r(self.xs) \n",
    "                                                            - tf.reshape(self.pde.f(self.npxs), shape=(self.snap-1,1) ) ))\n",
    "\n",
    "        # Add phi_b, phi_i, and phi_s to the loss\n",
    "        loss += phi_i + phi_s\n",
    "\n",
    "        return loss, phi_r, phi_i, phi_s\n",
    "\n",
    "\n",
    "    def loss_weak(self, x):\n",
    "\n",
    "        # Compute phi_r\n",
    "        r = self.get_residual(x)\n",
    "        phi_r = self.a * tf.reduce_mean(tf.square(r))\n",
    "\n",
    "        # Initialize loss with residual loss function\n",
    "        loss = phi_r\n",
    "\n",
    "        phi_b = 0\n",
    "        phi_i = 0\n",
    "        for i,model in enumerate(self.model_i):\n",
    "\n",
    "            b = self.xb[i]\n",
    "\n",
    "            # Calculate boundary loss for current model if applicable\n",
    "            if not model:\n",
    "                u_pred = self.model_r(b)\n",
    "                phi_b += (1 - self.a) * tf.reduce_mean(tf.square(self.pde.f_b(b) - u_pred))\n",
    "                continue\n",
    "\n",
    "            # Calculate interface loss for current model if applicable\n",
    "            u_pred1 = self.model_r(b)\n",
    "            u_pred2 = model[0](b)\n",
    "            phi_i += (1 - self.a) * tf.reduce_mean(tf.square(u_pred1 - u_pred2))\n",
    "\n",
    "        phi_s = 0\n",
    "        if self.snap:\n",
    "            # calculate snapshot data loss\n",
    "            phi_s = (1 - self.a) * tf.reduce_mean(tf.square( self.model_r(self.xs) \n",
    "                                                        - tf.reshape(self.pde.f(self.npxs), shape=(self.snap-1,1) ) ))\n",
    "\n",
    "        # Add phi_b, phi_i, and phi_s to the loss\n",
    "        loss += phi_b + phi_i + phi_s\n",
    "\n",
    "        return loss, phi_r, phi_b, phi_i, phi_s\n",
    "\n",
    "    \n",
    "    @tf.function\n",
    "    def get_gradient(self, x):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # This tape is for derivatives with respect to trainable variables\n",
    "            tape.watch(self.model_r.trainable_variables)\n",
    "            if self.strong:\n",
    "                loss, _, _, _ = self.loss_strong(x)\n",
    "            else:\n",
    "                loss, _, _, _, _ = self.loss_weak(x)\n",
    "\n",
    "        g = tape.gradient(loss, self.model_r.trainable_variables)\n",
    "\n",
    "        return g\n",
    "\n",
    "\n",
    "    def FD_update(self):\n",
    "\n",
    "        a, b, c, d = self.model_r.coeff\n",
    "        model = self.model_i\n",
    "        A = self.model_r.A\n",
    "\n",
    "        if (model[0] and model[1]):\n",
    "            f_NN = np.ones((A.shape[0], 1))\n",
    "\n",
    "            if (self.strong and not isinstance(model[0][0], FD_1D_Steady)):\n",
    "                f_NN[0] = f_NN[0] + ( -d*self.BC_enforce(self.x[0])*model[0][0](tf.reshape(self.x[0], shape=(1,1))) \n",
    "                             -c*self.BC_enforce(self.x[1])*model[0][0](tf.reshape(self.x[1], shape=(1,1))) )\n",
    "                f_NN[1] = f_NN[1] + ( -d*self.BC_enforce(self.x[1])*model[0][0](tf.reshape(self.x[1], shape=(1,1))) )\n",
    "\n",
    "            else:    \n",
    "                f_NN[0] = f_NN[0] + ( -d*model[0][0](tf.reshape(self.x[0], shape=(1,1))) \n",
    "                             -c*model[0][0](tf.reshape(self.x[1], shape=(1,1))) )\n",
    "                f_NN[1] = f_NN[1] + ( -d*model[0][0](tf.reshape(self.x[1], shape=(1,1))) )\n",
    "\n",
    "            if (self.strong and not isinstance(model[1][0], FD_1D_Steady)):\n",
    "                f_NN[-1] = f_NN[-1] + ( -a*self.BC_enforce(self.x[-1])*model[1][0]( tf.reshape(self.x[-1], shape=(1,1))) )\n",
    "            else:    \n",
    "                f_NN[-1] = f_NN[-1] + ( -a*model[1][0](tf.reshape(self.x[-1], shape=(1,1))) )\n",
    "\n",
    "            u_FD = np.squeeze( np.linalg.solve( A, f_NN ) )\n",
    "\n",
    "        elif model[1]:\n",
    "            f_NN = np.ones((A.shape[0]-1, 1))\n",
    "            A = A[:-1, :-1]\n",
    "\n",
    "            if (self.strong and not isinstance(model[1][0], FD_1D_Steady)):\n",
    "                f_NN[-1] = f_NN[-1] + ( -a*self.BC_enforce(self.x[-1])*model[1][0](tf.reshape(self.x[-1], shape=(1,1))) )\n",
    "            else:    \n",
    "                f_NN[-1] = f_NN[-1] + ( -a*model[1][0](tf.reshape(self.x[-1], shape=(1,1))) )\n",
    "\n",
    "            u_FD = np.linalg.solve( A, f_NN )\n",
    "\n",
    "            u_FD = np.hstack((self.pde.f(self.x[0]), u_FD.flatten()))\n",
    "\n",
    "        elif model[0]:\n",
    "            f_NN = np.ones((A.shape[0]-1, 1))\n",
    "            A = A[:-1, :-1]\n",
    "\n",
    "            if (self.strong and not isinstance(model[0][0], FD_1D_Steady)):\n",
    "                f_NN[0] = f_NN[0] + ( -d*self.BC_enforce(self.x[0])*model[0][0](tf.reshape(self.x[0], shape=(1,1))) \n",
    "                             -c*self.BC_enforce(self.x[1])*model[0][0](tf.reshape(self.x[1], shape=(1,1))) )\n",
    "                f_NN[1] = f_NN[1] + ( -d*self.BC_enforce(self.x[1])*model[0][0](tf.reshape(self.x[1], shape=(1,1))) )\n",
    "\n",
    "            else:    \n",
    "                f_NN[0] = f_NN[0] + ( -d*model[0][0](tf.reshape(self.x[0], shape=(1,1))) \n",
    "                             -c*model[0][0](tf.reshape(self.x[1], shape=(1,1))) )\n",
    "                f_NN[1] = f_NN[1] + ( -d*model[0][0](tf.reshape(self.x[1], shape=(1,1))) )\n",
    "\n",
    "            u_FD = np.linalg.solve( A, f_NN )\n",
    "\n",
    "            u_FD = np.hstack((u_FD.flatten(), self.pde.f(self.x[-1])))\n",
    "\n",
    "        else:\n",
    "\n",
    "            u_FD = np.squeeze(self.pde.f(self.x))\n",
    "\n",
    "        # Update u for current model\n",
    "        self.model_r.u = u_FD\n",
    "\n",
    "\n",
    "\n",
    "    def solve(self, optimizer, numEpochs):\n",
    "\n",
    "        @tf.function\n",
    "        def train_step(x):\n",
    "            # Retrieve loss gradient w.r.t. trainable variables\n",
    "            grad_theta = self.get_gradient(x)\n",
    "\n",
    "            # Perform gradient descent step\n",
    "            optimizer.apply_gradients(zip(grad_theta, self.model_r.trainable_variables))\n",
    "\n",
    "        # Split data into training batches\n",
    "        #train_dataset = tf.data.Dataset.from_tensor_slices((self.x,))\n",
    "        #train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "        # If current model is FOM, update interface boundaries with adjacent NN models\n",
    "        if isinstance(self.model_r, FD_1D_Steady):\n",
    "            self.FD_update()\n",
    "            self.err = np.square(self.model_r.u - self.pde.f(self.x)).mean()\n",
    "        else: \n",
    "            # Iterate training\n",
    "            for i in range(numEpochs):\n",
    "\n",
    "                # Train on each batch\n",
    "                #for (x_batch_train,) in train_dataset:\n",
    "                    #train_step(x_batch_train)\n",
    "                train_step(self.x)\n",
    "\n",
    "                # Compute loss for full dataset to track training progress\n",
    "                if self.strong:\n",
    "                    self.loss, self.phi_r, self.phi_i, self.phi_s = self.loss_strong(self.x)\n",
    "                else:\n",
    "                    self.loss, self.phi_r, self.phi_b, self.phi_i, self.phi_s = self.loss_weak(self.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9deeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A general class for FD models on subdomains\n",
    "class FD_1D_Steady():\n",
    "    def __init__(self, X, BC, pde, u0=np.random.rand(1), u1=np.random.rand(1), u_min1=np.random.rand(1)):\n",
    "\n",
    "        self.X = X\n",
    "        n_FD = len(X)\n",
    "        xl = X[0]\n",
    "        xr = X[-1]\n",
    "        \n",
    "        h = X[1]-X[0]\n",
    "        \n",
    "        nu = pde.nu\n",
    "        beta = pde.beta\n",
    "        order = pde.order\n",
    "        \n",
    "        a = - nu/(h**2)\n",
    "        b = (2*nu)/(h**2)\n",
    "        c = -(nu/(h**2))\n",
    "\n",
    "        if order == 1:\n",
    "            b += beta / h\n",
    "            c += -beta / h\n",
    "            d = 0.0\n",
    "        elif order == 2:\n",
    "            b += 3 / 2 * beta / h\n",
    "            c += -2 * beta / h\n",
    "            d = 1 / 2 * beta / h\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid order: {order}\")\n",
    "        \n",
    "        A = np.diagflat([b]*(n_FD)) + np.diagflat([c]*(n_FD - 1), -1) + np.diagflat([a]*(n_FD - 1), 1) + np.diagflat([d] * (n_FD - 2), -2)\n",
    "        \n",
    "         \n",
    "        if xr == BC[1]:\n",
    "            y = np.ones((A.shape[0]-1, 1)) \n",
    "            y[0] += -d*u0 - c*u1\n",
    "            y[1] += -d*u1\n",
    "\n",
    "            u_FD = np.linalg.solve( A[:-1, :-1], y )\n",
    "            self.u = np.hstack( (u_FD.flatten(), pde.f(xr)) )\n",
    "\n",
    "        elif xl == BC[0]:\n",
    "            y = np.ones((A.shape[0]-1, 1))\n",
    "            y[-1] += -a*u_min1\n",
    "\n",
    "            u_FD = np.linalg.solve( A[:-1, :-1], y )\n",
    "            self.u = np.hstack( (pde.f(xl), u_FD.flatten()) )\n",
    "\n",
    "        else:\n",
    "            y = np.ones((A.shape[0], 1))\n",
    "            y[0] += -d*u0 - c*u1\n",
    "            y[1] += -d*u1\n",
    "            y[-1] += -a*u_min1\n",
    "\n",
    "            u_FD = np.linalg.solve( A, y )\n",
    "            self.u = np.squeeze(u_FD)\n",
    "            \n",
    "        self.A = A\n",
    "        self.coeff = (a, b, c, d)\n",
    "        self.u0 = u0\n",
    "        self.u1 = u1\n",
    "        self.u_min1 = u_min1\n",
    "    \n",
    "    # Mimic functionality of model(x)\n",
    "    def __call__(self, x):\n",
    "        return np.interp(x, self.X, self.u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47f2641",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN_Pre_Train_g():\n",
    "    def __init__(self, pde, model_r, X_r, X_b, alpha, strong, snap):\n",
    "        \n",
    "        # Store PDE\n",
    "        self.pde = pde\n",
    "        \n",
    "        # Store models \n",
    "        self.model_r = model_r\n",
    "\n",
    "        # Store internal collocation points\n",
    "        self.x = X_r\n",
    "\n",
    "        # Store boundary points\n",
    "        self.xb = X_b\n",
    "        \n",
    "        # Store BC enforcement boolean\n",
    "        self.strong = strong\n",
    "        \n",
    "        # Store loss scaling coefficient\n",
    "        self.a = alpha\n",
    "        \n",
    "        # Store snapshot points if applicable\n",
    "        self.snap = snap[0]\n",
    "        if self.snap:\n",
    "            self.fd = snap[1]\n",
    "            self.us = tf.reshape(self.fd.u, shape=(self.snap,1) )\n",
    "            \n",
    "            self.xs = tf.concat([tf.constant(self.fd.X, shape=(self.snap, 1), dtype='float64'), \n",
    "                                tf.constant(np.tile(X_r[0][1:X_r.shape[1]], (self.snap, 1)), dtype='float64')], 1)\n",
    "        \n",
    "        # Initialize training loss and FD error storage\n",
    "        self.loss = 0\n",
    "        self.err = 0\n",
    "\n",
    "    \n",
    "    def BC_enforce(self, y):\n",
    "        x = tf.reshape(y[:,0], shape=(y.shape[0], 1))\n",
    "        return tf.math.tanh( (1-x) )*tf.math.tanh( x )\n",
    "\n",
    "    \n",
    "    @tf.function\n",
    "    def get_residual(self, x):\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Watch variable x during this GradientTape\n",
    "            tape.watch(x)\n",
    "\n",
    "            # Compute current values u(x) with strongly enforced BCs\n",
    "            if self.strong:\n",
    "                u = self.BC_enforce(x)*self.model_r(x)\n",
    "            else:\n",
    "                u = self.model_r(x)\n",
    "\n",
    "            # Store first derivative\n",
    "            u_x = tape.gradient(u, x)\n",
    "            \n",
    "        # Store second derivative \n",
    "        u_xx = tape.gradient(u_x, x)\n",
    "        \n",
    "        u_x = tf.reduce_sum(u_x, 1, keepdims=True)\n",
    "        u_xx = tf.reduce_sum(u_xx, 1, keepdims=True)\n",
    "        \n",
    "        del tape\n",
    "\n",
    "        return self.pde.f_r(u_x, u_xx)\n",
    "\n",
    "    \n",
    "    def loss_strong(self, x):\n",
    "\n",
    "        # Compute phi_r\n",
    "        r = self.get_residual(x)\n",
    "        phi_r = self.a * tf.reduce_mean(tf.square(r))\n",
    "\n",
    "        # Initialize loss with residual loss function\n",
    "        loss = phi_r\n",
    "        \n",
    "        phi_i = 0\n",
    "        for i,b in enumerate(self.xb):\n",
    "            bi = tf.gather(b, i+1, axis=1)\n",
    "            \n",
    "            u_pred = self.BC_enforce(b)*self.model_r(b)\n",
    "            phi_i += (1 - self.a) * tf.reduce_mean(tf.square(u_pred - bi))\n",
    "        \n",
    "        phi_s = 0\n",
    "        if self.snap:\n",
    "            # calculate snapshot data loss\n",
    "            phi_s = (1 - self.a) * tf.reduce_mean(tf.square( self.BC_enforce(self.xs)*self.model_r(self.xs) - self.us ) )\n",
    "\n",
    "        # Add phi_b, phi_i, and phi_s to the loss\n",
    "        loss += phi_i + phi_s\n",
    "\n",
    "        return loss, phi_r, phi_i, phi_s\n",
    "\n",
    "\n",
    "    def loss_weak(self, x):\n",
    "\n",
    "        # Compute phi_r\n",
    "        r = self.get_residual(x)\n",
    "        phi_r = self.a * tf.reduce_mean(tf.square(r))\n",
    "\n",
    "        # Initialize loss with residual loss function\n",
    "        loss = phi_r\n",
    "        \n",
    "        phi_b = 0\n",
    "        for i,b in enumerate(self.xb):\n",
    "            \n",
    "            bi = tf.gather(b, i+1, axis=1)\n",
    "            \n",
    "            u_pred = self.model_r(b)\n",
    "            phi_b += (1 - self.a) * tf.reduce_mean(tf.square(u_pred - bi))\n",
    "\n",
    "        phi_s = 0\n",
    "        if self.snap:\n",
    "            # calculate snapshot data loss\n",
    "            phi_s = (1 - self.a) * tf.reduce_mean(tf.square( self.model_r(self.xs) - self.us ))                                            \n",
    "\n",
    "        # Add phi_b, phi_i, and phi_s to the loss\n",
    "        loss += phi_b + phi_s\n",
    "\n",
    "        return loss, phi_r, phi_b, phi_s\n",
    "\n",
    "    \n",
    "    @tf.function\n",
    "    def get_gradient(self, x):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # This tape is for derivatives with respect to trainable variables\n",
    "            tape.watch(self.model_r.trainable_variables)\n",
    "            if self.strong:\n",
    "                loss, _, _, _ = self.loss_strong(x)\n",
    "            else:\n",
    "                loss, _, _, _ = self.loss_weak(x)\n",
    "\n",
    "        g = tape.gradient(loss, self.model_r.trainable_variables)\n",
    "\n",
    "        return g\n",
    "\n",
    "\n",
    "\n",
    "    def solve(self, optimizer, numEpochs, batch_size):\n",
    "\n",
    "        @tf.function\n",
    "        def train_step(x):\n",
    "            # Retrieve loss gradient w.r.t. trainable variables\n",
    "            grad_theta = self.get_gradient(x)\n",
    "\n",
    "            # Perform gradient descent step\n",
    "            optimizer.apply_gradients(zip(grad_theta, self.model_r.trainable_variables))\n",
    "        \n",
    "        if batch_size:\n",
    "            # Split data into training batches\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((self.x,))\n",
    "            train_dataset = train_dataset.shuffle(buffer_size=self.x.shape[0]).batch(batch_size)\n",
    "\n",
    "            # Iterate training\n",
    "            for i in range(numEpochs):\n",
    "                # Train on each batch\n",
    "                for (x_batch_train,) in train_dataset:\n",
    "                    train_step(x_batch_train)\n",
    "        else:\n",
    "            # Iterate training\n",
    "            for i in range(numEpochs):\n",
    "                train_step(self.x)\n",
    "\n",
    "        # Compute loss for full dataset to track training progress\n",
    "        if self.strong:\n",
    "            self.loss, self.phi_r, self.phi_i, self.phi_s = self.loss_strong(self.x)\n",
    "        else:\n",
    "            self.loss, self.phi_r, self.phi_b, self.phi_s = self.loss_weak(self.x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ROM_ML] *",
   "language": "python",
   "name": "conda-env-ROM_ML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
