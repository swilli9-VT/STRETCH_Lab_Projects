{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d1b2ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.animation import PillowWriter\n",
    "from matplotlib.animation import FFMpegWriter\n",
    "import nbimporter\n",
    "from PDE_Classes import *\n",
    "from PINN_Modular_Classes import *\n",
    "from silence_tensorflow import silence_tensorflow\n",
    "silence_tensorflow()\n",
    "import time\n",
    "# import itertools as product\n",
    "\n",
    "#point FFMpeg to the executable\n",
    "plt.rcParams['animation.ffmpeg_path'] = 'C:/Program Files/ImageMagick-7.1.1-Q16-HDRI/ffmpeg.exe'\n",
    "\n",
    "# Set data type\n",
    "DTYPE = 'float64'\n",
    "tf.keras.backend.set_floatx(DTYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db59159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize order parameter for PDE\n",
    "order = 2\n",
    "\n",
    "# number of internal collocation points\n",
    "N = 2**10\n",
    "# number of boundary and interface points\n",
    "N_b = 2**6 \n",
    "\n",
    "# Initialize list of points which lie on the system boundaries\n",
    "domain = [0, 1]\n",
    "\n",
    "# Set percentage overlap\n",
    "percent_overlap = 0.1\n",
    "beta = 1\n",
    "\n",
    "# Declare constant hyperparameters\n",
    "alpha = 0.2\n",
    "numEpochs = 2**10\n",
    "learn_rate = 5e-4#tf.keras.optimizers.schedules.PiecewiseConstantDecay([2**6],[5e-2,1e-3])\n",
    "schwarz_tol = 1e-3\n",
    "err_tol = 1e-2\n",
    "\n",
    "# This parameter is true when you are retrieving a file for a parameter sweep\n",
    "# Otherwise you are drawing a single test case from the manual test file\n",
    "retrieve = 0\n",
    "\n",
    "# Initialize dataframe for parameters and results storage\n",
    "if retrieve:\n",
    "    ParameterSweep = pd.read_csv(\"C:/Users/wdsnyde/code/fhnm-ldrd/Docs/Schwarz-PINNs/TrialCases3.csv\")\n",
    "else:\n",
    "    PS = pd.read_csv(\"C:/Users/wdsnyde/code/fhnm-ldrd/Docs/Schwarz-PINNs/Trial_Case_Manual_Test.csv\")\n",
    "    pos = 9+PS.loc[0, 'Sub-domains']\n",
    "    ParameterSweep = PS.drop(PS.iloc[:,pos:len(PS.columns)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b129ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter sweep loop\n",
    "for z in range(len(ParameterSweep.index)):\n",
    "    \n",
    "    # Set the number of subdomains and the desired percentage overlap\n",
    "    n_subdomains = ParameterSweep.loc[z, 'Sub-domains']\n",
    "    \n",
    "    BC_label = '_WDBC'\n",
    "    # Choose type of BC enforcement for system BCs (strong (1) or weak (0))\n",
    "    if ParameterSweep.loc[z, 'System BCs'] == 'weak':\n",
    "        sysBC = 0\n",
    "    elif ParameterSweep.loc[z, 'System BCs'] == 'strong':\n",
    "        sysBC = 1\n",
    "        \n",
    "    # Choose type of BC enforcement for Schwarz BCs (strong (1) or weak (0))\n",
    "    if ParameterSweep.loc[z, 'Schwarz BCs'] == 'weak':\n",
    "        schBC = 0\n",
    "    elif ParameterSweep.loc[z, 'Schwarz BCs'] == 'strong':\n",
    "        schBC = 1\n",
    "    \n",
    "    # assertion to catch SChwarz boundary enforcement when only domain is used\n",
    "    if schBC:\n",
    "        assert n_subdomains > 1, \"Schwarz boundaries cannot be strongly enforced because you are using a single-domain model which has no Schwarz boundaries.\"\n",
    "    \n",
    "    # Store BC enforcement booleans together\n",
    "    SDBC = [sysBC, schBC]\n",
    "    if all(SDBC):\n",
    "        BC_label = '_SDBC_both'\n",
    "    elif SDBC[0]:\n",
    "        BC_label = '_SDBC_sys'\n",
    "    elif SDBC[1]:\n",
    "        BC_label = '_SDBC_schwarz'\n",
    "    \n",
    "    NN_label = 'PINN'\n",
    "    # number of snapshots per subdomain used to aid training\n",
    "    if ParameterSweep.loc[z, 'Snapshots']:\n",
    "        snap = 2**6\n",
    "        NN_label = 'NN'\n",
    "    else:\n",
    "        snap=0\n",
    "        \n",
    "    # Set subdomains for FOM modeling, if any\n",
    "    FOM_label = ''\n",
    "    sub_FOM = np.zeros((n_subdomains,))\n",
    "    if ParameterSweep.loc[z, 'FOM'] == 'left':\n",
    "        sub_FOM[0] = 1\n",
    "        FOM_label = '_FOM_left'\n",
    "    elif ParameterSweep.loc[z, 'FOM'] == 'right':\n",
    "        sub_FOM[-1] = 1\n",
    "        FOM_label = '_FOM_right'\n",
    "\n",
    "    # Calculate the size of subdomains and overlap to achieve the parameters defined above \n",
    "    domain_size =  1 / ( n_subdomains*(1 - percent_overlap) + percent_overlap )\n",
    "    overlap = percent_overlap*domain_size\n",
    "\n",
    "    # Construct subdomain set\n",
    "    sub = ()\n",
    "    for i in range(n_subdomains):\n",
    "        step = i*(domain_size - overlap)\n",
    "        sub += ([step, step+domain_size],)\n",
    "\n",
    "    # Truncate subdomain boundaries to 8 decimal precision   \n",
    "    sub = np.round(sub, 8)\n",
    "\n",
    "    # Generate boundary points for each subdomain boundary\n",
    "    X_b_om = [ [ tf.constant(np.tile([i], (N_b, 1)), dtype=DTYPE) for i in sub[j] ] for j in range(len(sub))]\n",
    "\n",
    "    # Set random seed for reproducible results\n",
    "    tf.random.set_seed(0)\n",
    "    \n",
    "    # Declare nu based on Peclet number\n",
    "    Pe = ParameterSweep.loc[z, 'Peclet Number']\n",
    "    nu = 1/Pe\n",
    "    # Declare an instance of the PDE class \n",
    "    pde1 = PDE_1D_Steady_AdvecDiff(nu=nu, order=order)\n",
    "    \n",
    "    # FOM value \"true solution\" to use as a reference\n",
    "    x_true = tf.constant(np.linspace(domain[0], domain[1], num=N), shape=(N, 1), dtype=DTYPE)\n",
    "    u_true = pde1.f(x_true)\n",
    "\n",
    "    # Set number of FD points and initialize the step size\n",
    "    n_FD = int(N/2)\n",
    "    \n",
    "    # Set number of hidden layers and nodes per layer\n",
    "    hl = 2\n",
    "    nl = 20\n",
    "\n",
    "    # Initialize tuple to store internal points and models in each subdomain\n",
    "    X_r_om = ()\n",
    "    model_om = ()\n",
    "\n",
    "    # This alernative mode of quasi-random sample point generation may be still be used \n",
    "#     # Generate uniform internal points for the whole domain\n",
    "#     sampler = stats.qmc.Sobol(1, scramble=False)\n",
    "#     points = sampler.random(N)\n",
    "#     xwidth = domain[1] - domain[0]\n",
    "#     x = xwidth*points + domain[0]\n",
    "    \n",
    "    # Build and store neural networks for applicable subdomains; Store FD class for FOM sub-domains\n",
    "    for i in range(sub.shape[0]):\n",
    "\n",
    "        # If subdomain is to be modeled by NN, add random uniform points to subdomain, include sub-domain bounds\n",
    "        if not sub_FOM[i]:\n",
    "            xl = sub[i][0]\n",
    "            xr = sub[i][1]\n",
    "            temp = np.random.uniform(low=xl, high=xr, size=(N,))\n",
    "            temp = np.append(temp, sub[i])\n",
    "#             temp = (x[(x >= sub[i][0]) & (x <= sub[i][1])] - sub[0][0]) / (sub[-1][1] - sub[0][0])\n",
    "#             temp.reshape((len(temp),1))\n",
    "            X_r_om += ( tf.constant(temp, shape=(temp.shape[0],1), dtype=DTYPE), )\n",
    "            \n",
    "            model_om += ( PINN_Architecture(xl=xl, xr=xr, num_hidden_layers=hl, num_neurons_per_layer=nl), )\n",
    "            model_om[i].build(input_shape=(None, X_r_om[i].shape[1]))\n",
    "            \n",
    "            continue\n",
    "\n",
    "        # If subdomain is to be modeled by FD, construct uniform line space for FD on subdomain\n",
    "        xl = sub[i][0]\n",
    "        xr = sub[i][1]\n",
    "        x_FD = np.linspace(xl, xr, num=n_FD)\n",
    "\n",
    "        # Add line space for subdomain to internal point storage\n",
    "        X_r_om += ( tf.constant(x_FD, shape=(x_FD.shape[0],1), dtype=DTYPE), )\n",
    "\n",
    "        model = FD_1D_Steady(x_FD, domain, pde1)\n",
    "\n",
    "        model_om += ( model, )\n",
    "        \n",
    "        \n",
    "    # Initialize schwarz loop operators\n",
    "    schwarz_conv = 1\n",
    "    ref_err = 1\n",
    "    iterCount = 0\n",
    "    x_schwarz = [tf.constant(np.linspace(s[0], s[1], num=n_FD), shape=(n_FD, 1), dtype=DTYPE) for s in sub]\n",
    "    u_i_minus1 = [tf.constant(np.random.rand(n_FD,1), shape=(n_FD, 1), dtype=DTYPE) for _ in x_schwarz]\n",
    "    u_i = [tf.constant(np.zeros((n_FD,1)), shape=(n_FD, 1), dtype=DTYPE) for _ in x_schwarz]\n",
    "    \n",
    "    # Initialize variables for plotting Schwarz results\n",
    "    fig = plt.figure(layout='tight')\n",
    "    #fig.tight_layout(pad=1.0)\n",
    "    fig.set_size_inches(6, 3)\n",
    "    plt.xlabel('x', fontsize=14)\n",
    "    plt.ylabel('u(x)', fontsize=14)\n",
    "    ref, = plt.plot(x_true, u_true, 'k--')\n",
    "    subdomain_plots = [plt.plot([], []) for _ in range(n_subdomains)]\n",
    "    \n",
    "    #Set animation parameters\n",
    "    fps = 6    \n",
    "    metadata = dict(title='SchwarzRecord', author='Will Snyder')\n",
    "    #writer = PillowWriter(fps=fps, metadata=metadata)\n",
    "    writer = FFMpegWriter(fps=fps, metadata=metadata)\n",
    "    \n",
    "    # Declare writer to begin recording animations\n",
    "    with writer.saving(fig, NN_label+BC_label+FOM_label+\"_Pe_{:d}_nSub_{:d}.mp4\".format(int(beta/nu), n_subdomains), 600):\n",
    "        \n",
    "        # Begin recording time\n",
    "        start = time.time()\n",
    "        \n",
    "        # Record u_hat at each boundary to stack on each iteration\n",
    "        u_gamma = np.zeros(sub.shape)\n",
    "        \n",
    "        # Main Schwarz loop\n",
    "        while (schwarz_conv > schwarz_tol or ref_err > err_tol):\n",
    "\n",
    "            # initialize error check variables to 0 as they are to be added to during sub-domain iteration\n",
    "            schwarz_conv = 0\n",
    "            ref_err = 0\n",
    "            \n",
    "            # Add to Schwarz iteration count\n",
    "            iterCount += 1\n",
    "            \n",
    "            # Update title for Schwarz iter\n",
    "            plt.title('Schwarz iteration {:d}; Pe = {:d}'.format(iterCount, Pe), fontsize=14)\n",
    "\n",
    "            # loop over each model for training\n",
    "            for s in range(len(model_om)):\n",
    "\n",
    "                # Current model domain points\n",
    "                X_r = X_r_om[s]\n",
    "                # Current model boundary points\n",
    "                X_b = X_b_om[s]\n",
    "\n",
    "                # Current model\n",
    "                model_r = model_om[s]\n",
    "                # Adjacent models for interface conditions\n",
    "                model_i = [model_om[s-1:s], model_om[s+1:s+2]]\n",
    "                \n",
    "                # update the current models BCs according to adjacent models and save to model for SDBCs\n",
    "                if any(SDBC):\n",
    "                    if model_i[0]:\n",
    "                        u_gamma[s][0] = np.interp(sub[s][0], x_schwarz[s-1][:,0], u_i[s-1][:,0])\n",
    "                    if model_i[1]:\n",
    "                        u_gamma[s][1] = np.interp(sub[s][1], x_schwarz[s+1][:,0], u_i[s+1][:,0])\n",
    "                model_r.u_gamma = u_gamma[s]\n",
    "                \n",
    "                # Initialize solver\n",
    "                p = PINN_Schwarz_Steady(pde1, model_r, model_i, SDBC, X_r, X_b, alpha, snap)\n",
    "\n",
    "                # Solve model for current sub-domain\n",
    "                p.solve(tf.keras.optimizers.Adam(learning_rate=learn_rate), numEpochs)\n",
    "\n",
    "                MSE_tag = \"MSE Sub-domain {:d}\".format(s+1)\n",
    "                \n",
    "                # If current model is FD, output FD Error, update current iteration solution and convergence metrics\n",
    "                if isinstance(model_r, FD_1D_Steady):\n",
    "                    print('Model {:d}: '.format(s+1))\n",
    "                    print('\\t'+'Finite Difference error = {:10.8e}'.format(p.err))\n",
    "\n",
    "                    ParameterSweep.loc[z, MSE_tag] = np.float64(p.err)\n",
    "\n",
    "                    u_i[s] = model_r(x_schwarz[s])\n",
    "                    schwarz_conv += tf.math.reduce_euclidean_norm(u_i[s] - u_i_minus1[s])/tf.math.reduce_euclidean_norm(u_i[s])\n",
    "                    ref_err += tf.math.reduce_euclidean_norm(u_i[s] - pde1.f(x_schwarz[s]))/tf.math.reduce_euclidean_norm(pde1.f(x_schwarz[s]))\n",
    "                \n",
    "                # If current model is NN output the model loss, update current iteration solution and convergence metrics\n",
    "                else:\n",
    "                    print('Model {:d}: '.format(s+1))\n",
    "                    print('\\t'+'Residual loss = {:10.8e}'.format(p.phi_r))\n",
    "                    if not schBC:\n",
    "                        print('\\t'+'Interface loss = {:10.8e}'.format(p.phi_i))\n",
    "                    if not sysBC:\n",
    "                        print('\\t'+'Boundary loss = {:10.8e}'.format(p.phi_b))\n",
    "                    if snap:\n",
    "                        print('\\t'+'Snapshot loss = {:10.8e}'.format(p.phi_s))\n",
    "                    print('\\t'+'Total loss = {:10.8e}'.format(p.loss))\n",
    "\n",
    "                    ParameterSweep.loc[z, MSE_tag] = np.float64(p.loss)\n",
    "\n",
    "                    if any(SDBC):\n",
    "                        u_i[s] = p.get_u_hat(x_schwarz[s], model_r)\n",
    "                    else:\n",
    "                        u_i[s] = model_r(x_schwarz[s])\n",
    "                        \n",
    "                    schwarz_conv += tf.math.reduce_euclidean_norm(u_i[s] - u_i_minus1[s])/tf.math.reduce_euclidean_norm(u_i[s])\n",
    "                    ref_err += tf.math.reduce_euclidean_norm(u_i[s] - pde1.f(x_schwarz[s]))/tf.math.reduce_euclidean_norm(pde1.f(x_schwarz[s]))\n",
    "                \n",
    "                \n",
    "                # update frame for animation\n",
    "                subdomain_plots[s][0].set_data(x_schwarz[s][:,0], u_i[s])\n",
    "                \n",
    "                # Gather frame for animation\n",
    "                writer.grab_frame()\n",
    "            \n",
    "            \n",
    "            # Calculate the normalized difference between u for the current iteration and u for the previous iteration\n",
    "            schwarz_conv = schwarz_conv/len(u_i)\n",
    "            ref_err = ref_err/len(u_i)\n",
    "\n",
    "            # Update the value of u stored for the previous iteration\n",
    "            u_i_minus1 = u_i.copy()\n",
    "            \n",
    "            # Save current frame as image if needed for beamer animations\n",
    "            #fig.savefig(\"Frames/SchwarzPINN_WDBC_Pe_{:d}_nSub_{:d}-{:d}.pdf\".format(int(beta/nu), n_subdomains, iterCount),dpi=600)\n",
    "            \n",
    "            # Output current Schwarz error \n",
    "            print('\\nSchwarz iteration {:d}: Convergence error = {:10.8e}, Reference Error = {:10.8e}'.format(iterCount, schwarz_conv, ref_err), \"\\n\")\n",
    "\n",
    "            # Cut off simulat at a particular number of Schwarz iterations\n",
    "            if (iterCount == 100):\n",
    "                break\n",
    "        \n",
    "        # End time recording\n",
    "        end = time.time()\n",
    "        \n",
    "        # Capture 2 more seconds on final frame to pad out video\n",
    "        for _ in range(fps*2):\n",
    "            writer.grab_frame()\n",
    "        \n",
    "        # Record results for coupled model\n",
    "        ParameterSweep.loc[z, 'CPU Time (s)'] = end-start\n",
    "        ParameterSweep.loc[z, 'Schwarz Iterations'] = iterCount\n",
    "        ParameterSweep.loc[z, 'Avg L2 Error'] = np.float64(ref_err)\n",
    "\n",
    "# Export final results to CSV \n",
    "ParameterSweep.to_csv(\"C:/Users/wdsnyde/code/fhnm-ldrd/Docs/Schwarz-PINNs/ParamResults.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandia",
   "language": "python",
   "name": "sandia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
