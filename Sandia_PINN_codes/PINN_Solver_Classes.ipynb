{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb5c55e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nbimporter\n",
    "from PDE_Classes import *\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "\n",
    "# A general class for constructing neural network architecture\n",
    "class PINN_Architecture(tf.keras.Model):\n",
    "    def __init__(self, x0, x1, \n",
    "            num_hidden_layers=2, \n",
    "            num_neurons_per_layer=20,\n",
    "            output_dim=1,\n",
    "            activation=tf.keras.activations.swish,\n",
    "            kernel_initializer='glorot_normal',\n",
    "            **kwargs):\n",
    "        \n",
    "        # Intialize superclass with its default parameter signature\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Store hyperparameters\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.x0 = x0\n",
    "        self.x1 = x1\n",
    "        \n",
    "        # Define NN architecture\n",
    "        self.scale = tf.keras.layers.Lambda(\n",
    "            lambda x: (x - x0)/(x1 - x0))\n",
    "        self.hidden = [tf.keras.layers.Dense(num_neurons_per_layer,\n",
    "                             activation=tf.keras.activations.get(activation),\n",
    "                             kernel_initializer=kernel_initializer)\n",
    "                           for _ in range(self.num_hidden_layers)]\n",
    "        self.out = tf.keras.layers.Dense(output_dim)\n",
    "        \n",
    "    \n",
    "    # Mimic functionality of model(x)\n",
    "    def call(self, X):\n",
    "        #Forward-pass through neural network.\n",
    "        Z = self.scale(X)\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            Z = self.hidden[i](Z)\n",
    "        return self.out(Z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93426a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A general class for FD models on subdomains\n",
    "class FD_1D_Steady():\n",
    "    def __init__(self, X, BC, pde):\n",
    "\n",
    "        self.X = X\n",
    "        n_FD = len(X)\n",
    "        xl = X[0]\n",
    "        xr = X[-1]\n",
    "        \n",
    "        h = X[1]-X[0]\n",
    "        \n",
    "        nu = pde.nu\n",
    "        beta = pde.beta\n",
    "        order = pde.order\n",
    "        \n",
    "        a = - nu/(h**2)\n",
    "        b = (2*nu)/(h**2)\n",
    "        c = -(nu/(h**2))\n",
    "\n",
    "        if order == 1:\n",
    "            b += beta / h\n",
    "            c += -beta / h\n",
    "            d = 0.0\n",
    "        elif order == 2:\n",
    "            b += 3 / 2 * beta / h\n",
    "            c += -2 * beta / h\n",
    "            d = 1 / 2 * beta / h\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid order: {order}\")\n",
    "        \n",
    "        A = np.diagflat([b]*(n_FD)) + np.diagflat([c]*(n_FD - 1), -1) + np.diagflat([a]*(n_FD - 1), 1) + np.diagflat([d] * (n_FD - 2), -2)\n",
    "\n",
    "        if xr == BC[1]:\n",
    "            y = np.ones((A.shape[0]-1, 1)) \n",
    "            y[0] += -d*np.random.rand(1) - c*np.random.rand(1)\n",
    "            y[1] += -d*np.random.rand(1)\n",
    "\n",
    "            u_FD = np.linalg.solve( A[:-1, :-1], y )\n",
    "            self.u = np.hstack( (u_FD.flatten(), pde.f(xr)) )\n",
    "\n",
    "        elif xl == BC[0]:\n",
    "            y = np.ones((A.shape[0]-1, 1))\n",
    "            y[-1] += -a*np.random.rand(1)\n",
    "\n",
    "            u_FD = np.linalg.solve( A[:-1, :-1], y )\n",
    "            self.u = np.hstack( (pde.f(xl), u_FD.flatten()) )\n",
    "\n",
    "        else:\n",
    "            y = np.ones((A.shape[0], 1))\n",
    "            y[0] += -d*np.random.rand(1) - c*np.random.rand(1)\n",
    "            y[1] += -d*np.random.rand(1)\n",
    "            y[-1] += -a*np.random.rand(1)\n",
    "\n",
    "            u_FD = np.linalg.solve( A, y )\n",
    "            self.u = np.squeeze(u_FD)\n",
    "            \n",
    "        self.A = A\n",
    "        self.coeff = (a, b, c, d)\n",
    "        \n",
    "#     def update_u(self, U):\n",
    "#         self.u = U\n",
    "    \n",
    "    # Mimic functionality of model(x)\n",
    "    def __call__(self, x):\n",
    "        return np.interp(x, self.X, self.u)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae14271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Multiple domain Schwarz Coupling of a PINN with a finite difference model using both SDBCs and WDBCs\n",
    "\n",
    "# Shell function used for dynamic class inheritance of PDEs\n",
    "def PINN_Schwarz_Steady(model_r, model_i, X_r, X_b, alpha, pde, strong, snap=0, **kwargs):\n",
    "    \n",
    "    \n",
    "    class PINN_Solver_Schwarz(pde):\n",
    "        def __init__(self, model_r, model_i, X_r, X_b, alpha, strong, snap, **kwargs):\n",
    "            \n",
    "            # Intialize dynamic superclass with its default parameter signature\n",
    "            super().__init__(**kwargs)\n",
    "            \n",
    "            # Store models \n",
    "            self.model_r = model_r\n",
    "            self.model_i = model_i\n",
    "\n",
    "            # Store internal collocation points\n",
    "            self.x = X_r\n",
    "\n",
    "            # Store boundary points\n",
    "            self.xb = X_b\n",
    "            \n",
    "            # Store snapshot points if applicable\n",
    "            if snap:\n",
    "                self.xs = tf.constant(np.linspace(float(self.xb[0][0][0]), float(self.xb[1][0][0]), num=snap, \n",
    "                                    endpoint=False)[1:], shape=(snap-1, 1), dtype='float64')\n",
    "\n",
    "            # Store loss scaling coefficient\n",
    "            self.a = alpha\n",
    "            \n",
    "            self.loss = 0\n",
    "            self.err = 0\n",
    "    \n",
    "    \n",
    "        def BC_enforce(self, x):\n",
    "            return (tf.math.tanh( 5*(1-x) )*tf.math.tanh( x ))\n",
    "    \n",
    "    \n",
    "        def get_residual(self, x):\n",
    "\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                # Watch variable x during this GradientTape\n",
    "                tape.watch(x)\n",
    "                \n",
    "                # Compute current values u(x) with strongly enforced BCs\n",
    "                if strong:\n",
    "                    u = self.BC_enforce(x)*self.model_r(x)\n",
    "                else:\n",
    "                    u = self.model_r(x)\n",
    "                \n",
    "                # Store first derivative\n",
    "                u_x = tape.gradient(u, x)\n",
    "            \n",
    "            # Store second derivative \n",
    "            u_xx = tape.gradient(u_x, x)\n",
    "            del tape\n",
    "\n",
    "            return self.f_r(u_x, u_xx)\n",
    "        \n",
    "\n",
    "        def loss_strong(self, x):\n",
    "\n",
    "            # Compute phi_r\n",
    "            r = self.get_residual(x)\n",
    "            phi_r = self.a * tf.reduce_mean(tf.square(r))\n",
    "\n",
    "            # Initialize loss with residual loss function\n",
    "            loss = phi_r\n",
    "            \n",
    "            phi_i = 0\n",
    "            for i,model in enumerate(self.model_i):\n",
    "                if not model:\n",
    "                    continue\n",
    "                \n",
    "                b = self.xb[i]\n",
    "                \n",
    "                # Calculate interface loss for current model if applicable\n",
    "                u_pred1 = self.BC_enforce(b)*self.model_r(b)\n",
    "                if isinstance(model[0], FD_1D_Steady):\n",
    "                    u_pred2 = model[0](b)\n",
    "                else:\n",
    "                    u_pred2 = self.BC_enforce(b)*model[0](b)   \n",
    "                phi_i += (1 - self.a) * tf.reduce_mean(tf.square(u_pred1 - u_pred2))\n",
    "            \n",
    "            phi_s = 0\n",
    "            if snap:\n",
    "                # calculate snapshot data loss\n",
    "                phi_s = (1 - self.a) * tf.reduce_mean(tf.square( self.BC_enforce(self.xs)*self.model_r(self.xs) \n",
    "                                                                - self.f(self.xs) ))\n",
    "            \n",
    "            # Add phi_b, phi_i, and phi_s to the loss\n",
    "            loss += phi_i + phi_s\n",
    "            \n",
    "            return loss, phi_r, phi_i, phi_s\n",
    "        \n",
    "        \n",
    "        def loss_weak(self, x):\n",
    "\n",
    "            # Compute phi_r\n",
    "            r = self.get_residual(x)\n",
    "            phi_r = self.a * tf.reduce_mean(tf.square(r))\n",
    "            \n",
    "            # Initialize loss with residual loss function\n",
    "            loss = phi_r\n",
    "            \n",
    "            phi_b = 0\n",
    "            phi_i = 0\n",
    "            for i,model in enumerate(self.model_i):\n",
    "                \n",
    "                b = self.xb[i]\n",
    "                \n",
    "                # Calculate boundary loss for current model if applicable\n",
    "                if not model:\n",
    "                    u_pred = self.model_r(b)\n",
    "                    phi_b += (1 - self.a) * tf.reduce_mean(tf.square(self.f_b(b) - u_pred))\n",
    "                    continue\n",
    "                \n",
    "                # Calculate interface loss for current model if applicable\n",
    "                u_pred1 = self.model_r(b)\n",
    "                u_pred2 = model[0](b)\n",
    "                phi_i += (1 - self.a) * tf.reduce_mean(tf.square(u_pred1 - u_pred2))\n",
    "            \n",
    "            phi_s = 0\n",
    "            if snap:\n",
    "                # calculate snapshot data loss\n",
    "                phi_s = self.a * tf.reduce_mean(tf.square( self.model_r(self.xs) - self.f(self.xs) ))\n",
    "            \n",
    "            # Add phi_b, phi_i, and phi_s to the loss\n",
    "            loss += phi_b + phi_i + phi_s\n",
    "            \n",
    "            return loss, phi_r, phi_b, phi_i, phi_s\n",
    "        \n",
    "\n",
    "        def get_gradient(self, x):\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                # This tape is for derivatives with respect to trainable variables\n",
    "                tape.watch(self.model_r.trainable_variables)\n",
    "                if strong:\n",
    "                    loss, _, _, _ = self.loss_strong(x)\n",
    "                else:\n",
    "                    loss, _, _, _, _ = self.loss_weak(x)\n",
    "\n",
    "            g = tape.gradient(loss, self.model_r.trainable_variables)\n",
    "            del tape\n",
    "\n",
    "            return g\n",
    "        \n",
    "        \n",
    "        \n",
    "        def FD_update(self):\n",
    "            \n",
    "            a, b, c, d = self.model_r.coeff\n",
    "            model = self.model_i\n",
    "            A = self.model_r.A\n",
    "            \n",
    "            if (model[0] and model[1]):\n",
    "                f_NN = np.ones((A.shape[0], 1))\n",
    "                \n",
    "                if (strong and not isinstance(model[0][0], FD_1D_Steady)):\n",
    "                    f_NN[0] = f_NN[0] + ( -d*self.BC_enforce(self.x[0])*model[0][0](tf.reshape(self.x[0], shape=(1,1))) \n",
    "                                 -c*self.BC_enforce(self.x[1])*model[0][0](tf.reshape(self.x[1], shape=(1,1))) )\n",
    "                    f_NN[1] = f_NN[1] + ( -d*self.BC_enforce(self.x[1])*model[0][0](tf.reshape(self.x[1], shape=(1,1))) )\n",
    "                      \n",
    "                else:    \n",
    "                    f_NN[0] = f_NN[0] + ( -d*model[0][0](tf.reshape(self.x[0], shape=(1,1))) \n",
    "                                 -c*model[0][0](tf.reshape(self.x[1], shape=(1,1))) )\n",
    "                    f_NN[1] = f_NN[1] + ( -d*model[0][0](tf.reshape(self.x[1], shape=(1,1))) )\n",
    "                    \n",
    "                if (strong and not isinstance(model[1][0], FD_1D_Steady)):\n",
    "                    f_NN[-1] = f_NN[-1] + ( -a*self.BC_enforce(self.x[-1])*model[1][0]( tf.reshape(self.x[-1], shape=(1,1))) )\n",
    "                else:    \n",
    "                    f_NN[-1] = f_NN[-1] + ( -a*model[1][0](tf.reshape(self.x[-1], shape=(1,1))) )\n",
    "                    \n",
    "                u_FD = np.squeeze( np.linalg.solve( A, f_NN ) )\n",
    " \n",
    "            elif model[1]:\n",
    "                f_NN = np.ones((A.shape[0]-1, 1))\n",
    "                A = A[:-1, :-1]\n",
    "                \n",
    "                if (strong and not isinstance(model[1][0], FD_1D_Steady)):\n",
    "                    f_NN[-1] = f_NN[-1] + ( -a*self.BC_enforce(self.x[-1])*model[1][0](tf.reshape(self.x[-1], shape=(1,1))) )\n",
    "                else:    \n",
    "                    f_NN[-1] = f_NN[-1] + ( -a*model[1][0](tf.reshape(self.x[-1], shape=(1,1))) )\n",
    "\n",
    "                u_FD = np.linalg.solve( A, f_NN )\n",
    "                \n",
    "                u_FD = np.hstack((self.f(self.x[0]), u_FD.flatten()))\n",
    "            \n",
    "            elif model[0]:\n",
    "                f_NN = np.ones((A.shape[0]-1, 1))\n",
    "                A = A[:-1, :-1]\n",
    "                \n",
    "                if (strong and not isinstance(model[0][0], FD_1D_Steady)):\n",
    "                    f_NN[0] = f_NN[0] + ( -d*self.BC_enforce(self.x[0])*model[0][0](tf.reshape(self.x[0], shape=(1,1))) \n",
    "                                 -c*self.BC_enforce(self.x[1])*model[0][0](tf.reshape(self.x[1], shape=(1,1))) )\n",
    "                    f_NN[1] = f_NN[1] + ( -d*self.BC_enforce(self.x[1])*model[0][0](tf.reshape(self.x[1], shape=(1,1))) )\n",
    "                      \n",
    "                else:    \n",
    "                    f_NN[0] = f_NN[0] + ( -d*model[0][0](tf.reshape(self.x[0], shape=(1,1))) \n",
    "                                 -c*model[0][0](tf.reshape(self.x[1], shape=(1,1))) )\n",
    "                    f_NN[1] = f_NN[1] + ( -d*model[0][0](tf.reshape(self.x[1], shape=(1,1))) )\n",
    "                    \n",
    "                u_FD = np.linalg.solve( A, f_NN )\n",
    "                \n",
    "                u_FD = np.hstack((u_FD.flatten(), self.f(self.x[-1])))\n",
    "                    \n",
    "            else:\n",
    "\n",
    "                u_FD = np.squeeze(self.f(self.x))\n",
    "            \n",
    "            # Update u for current model\n",
    "            self.model_r.u = u_FD\n",
    "        \n",
    "        \n",
    "        \n",
    "        def solve(self, optimizer, batch_size, numEpochs):\n",
    "\n",
    "            @tf.function\n",
    "            def train_step(x):\n",
    "                # Retrieve loss gradient w.r.t. trainable variables\n",
    "                grad_theta = self.get_gradient(x)\n",
    "\n",
    "                # Perform gradient descent step\n",
    "                optimizer.apply_gradients(zip(grad_theta, self.model_r.trainable_variables))\n",
    "            \n",
    "            # Split data into training batches\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((self.x,))\n",
    "            train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "            \n",
    "            # If current model is FOM, update interface boundaries with adjacent NN models\n",
    "            if isinstance(self.model_r, FD_1D_Steady):\n",
    "                self.FD_update()\n",
    "                self.err = np.square(self.model_r.u - self.f(self.x)).mean()\n",
    "            else: \n",
    "                # Iterate training\n",
    "                for i in range(numEpochs):\n",
    "\n",
    "                    # Train on each batch\n",
    "                    for (x_batch_train,) in train_dataset:\n",
    "                        train_step(x_batch_train)\n",
    "\n",
    "                    # Compute loss for full dataset to track training progress\n",
    "                    if strong:\n",
    "                        self.loss, self.phi_r, self.phi_i, self.phi_s = self.loss_strong(self.x)\n",
    "                    else:\n",
    "                        self.loss, self.phi_r, self.phi_b, self.phi_i, self.phi_s = self.loss_weak(self.x)\n",
    "    \n",
    "    # Return intialized class instance\n",
    "    return PINN_Solver_Schwarz(model_r, model_i, X_r, X_b, alpha, strong, snap, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37150a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Single domain PINN for steady state strong form PDEs\n",
    "\n",
    "# Shell function used for dynamic class inheritance of PDEs\n",
    "def PINN_Solver_Steady(model, X_r, X_b, U_b, alpha, pde, **kwargs):\n",
    "\n",
    "    class PINN_Solver(pde):\n",
    "        def __init__(self, model, X_r, X_b, U_b, alpha, **kwargs):\n",
    "            \n",
    "            # Intialize dynamic superclass with its default parameter signature\n",
    "            super().__init__(**kwargs)\n",
    "            \n",
    "            # Store model \n",
    "            self.model = model\n",
    "\n",
    "            # Store internal collocation points\n",
    "            self.x = X_r\n",
    "\n",
    "            # Store boundary points\n",
    "            self.xb = X_b\n",
    "\n",
    "            # Use PDE to get boundary condition data\n",
    "            self.ub = U_b\n",
    "\n",
    "            # Store loss scaling coefficient\n",
    "            self.a = alpha\n",
    "\n",
    "            # Initialize history of losses and global iteration counter\n",
    "            self.hist = []\n",
    "            self.iter = 0\n",
    "\n",
    "        def get_residual(self, x):\n",
    "\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                # Watch variable x during this GradientTape\n",
    "                tape.watch(x)\n",
    "\n",
    "                # Compute current values u(x)\n",
    "                u = self.model(x)\n",
    "                \n",
    "                # Store first derivative\n",
    "                u_x = tape.gradient(u, x)\n",
    "            \n",
    "            # Store second derivative \n",
    "            u_xx = tape.gradient(u_x, x)\n",
    "            del tape\n",
    "\n",
    "            return self.f_r(u_x, u_xx)\n",
    "\n",
    "        def loss_function(self, x):\n",
    "\n",
    "            # Compute phi_r\n",
    "            r = self.get_residual(x)\n",
    "            phi_r = tf.reduce_mean(tf.square(r))\n",
    "\n",
    "            # Initialize loss\n",
    "            loss = self.a * phi_r\n",
    "\n",
    "            # Add phi_b to the loss\n",
    "            for i in range(2):\n",
    "                u_pred = self.model(self.xb[i])\n",
    "                loss += (1 - self.a) * tf.reduce_mean(tf.square(self.ub[i] - u_pred))\n",
    "\n",
    "            return loss\n",
    "\n",
    "        def get_gradient(self, x):\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                # This tape is for derivatives with respect to trainable variables\n",
    "                tape.watch(self.model.trainable_variables)\n",
    "                loss = self.loss_function(x)\n",
    "\n",
    "            g = tape.gradient(loss, self.model.trainable_variables)\n",
    "            del tape\n",
    "\n",
    "            return g\n",
    "\n",
    "\n",
    "        def solve(self, optimizer, batch_size, numEpochs):\n",
    "\n",
    "            @tf.function\n",
    "            def train_step(x):\n",
    "                # Retrieve loss gradient w.r.t. trainable variables\n",
    "                grad_theta = self.get_gradient(x)\n",
    "\n",
    "                # Perform gradient descent step\n",
    "                optimizer.apply_gradients(zip(grad_theta, self.model.trainable_variables))\n",
    "            \n",
    "            # Split data into training batches\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((self.x,))\n",
    "            train_dataset = train_dataset.shuffle(buffer_size=2048).batch(batch_size)\n",
    "            \n",
    "            # Iterate training\n",
    "            for i in range(numEpochs):\n",
    "                \n",
    "                # Train on each batch\n",
    "                for step, (x_batch_train,) in enumerate(train_dataset):\n",
    "                    train_step(x_batch_train)\n",
    "                \n",
    "                # Compute loss for full dataset to track training progress\n",
    "                loss = self.loss_function(self.x)\n",
    "                \n",
    "                # Append current loss to history\n",
    "                self.hist.append(loss.numpy())\n",
    "\n",
    "                # Output current loss after 2^5 epochs\n",
    "                if i%512 == 0:\n",
    "                    print('Epoch {:5d}: loss = {:10.8e}'.format(i,loss))\n",
    "    \n",
    "    # Return intialized class instance\n",
    "    return PINN_Solver(model, X_r, X_b, U_b, alpha, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cda712",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Multiple domain Schwarz Decomposition of PINNs for steady state strong form PDEs\n",
    "\n",
    "# Shell function used for dynamic class inheritance of PDEs\n",
    "def PINN_Solver_Schwarz_Steady(model_r, model_i, X_r, X_b, alpha, pde, snap=0, **kwargs):\n",
    "    \n",
    "    class PINN_Solver_Schwarz(pde):\n",
    "        def __init__(self, model_r, model_i, X_r, X_b, alpha, snap, **kwargs):\n",
    "            \n",
    "            # Intialize dynamic superclass with its default parameter signature\n",
    "            super().__init__(**kwargs)\n",
    "            \n",
    "            # Store models \n",
    "            self.model_r = model_r\n",
    "            self.model_i = model_i\n",
    "\n",
    "            # Store internal collocation points\n",
    "            self.x = X_r\n",
    "\n",
    "            # Store boundary points\n",
    "            self.xb = X_b\n",
    "            \n",
    "            # Store snapshot points if applicable\n",
    "            if snap:\n",
    "                self.xs = tf.constant(np.linspace(float(self.xb[0][0][0]), float(self.xb[1][0][0]), num=snap, \n",
    "                                    endpoint=False)[1:], shape=(snap-1, 1), dtype='float64')\n",
    "\n",
    "            # Store loss scaling coefficient\n",
    "            self.a = alpha\n",
    "\n",
    "            # Initialize history of losses and global iteration counter\n",
    "#             self.hist = []\n",
    "#             self.iter = 0\n",
    "            \n",
    "        \n",
    "        def get_residual(self, x):\n",
    "\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                # Watch variable x during this GradientTape\n",
    "                tape.watch(x)\n",
    "\n",
    "                # Compute current values u(x)\n",
    "                u = self.model_r(x)\n",
    "                \n",
    "                # Store first derivative\n",
    "                u_x = tape.gradient(u, x)\n",
    "            \n",
    "            # Store second derivative \n",
    "            u_xx = tape.gradient(u_x, x)\n",
    "            del tape\n",
    "\n",
    "            return self.f_r(u_x, u_xx)\n",
    "        \n",
    "\n",
    "        def loss_function(self, x):\n",
    "\n",
    "            # Compute phi_r\n",
    "            r = self.get_residual(x)\n",
    "            phi_r = self.a * tf.reduce_mean(tf.square(r))\n",
    "\n",
    "            # Initialize loss with residual loss function\n",
    "            loss = phi_r\n",
    "            \n",
    "            i=0\n",
    "            phi_b = 0\n",
    "            phi_i = 0\n",
    "            for b,y in self.xb:\n",
    "                # Calculate boundary loss for current model if applicable\n",
    "                if y:\n",
    "                    u_pred = self.model_r(b)\n",
    "                    phi_b += (1 - self.a) * tf.reduce_mean(tf.square(self.f_b(b) - u_pred))\n",
    "                    continue\n",
    "            \n",
    "                # Calculate interface loss for current model if applicable\n",
    "                u_pred1 = self.model_r(b)\n",
    "                u_pred2 = self.model_i[i](b)\n",
    "                phi_i += (1 - self.a) * tf.reduce_mean(tf.square(u_pred1 - u_pred2))\n",
    "                i += 1\n",
    "            \n",
    "            phi_s = 0\n",
    "            if snap:\n",
    "                # calculate snapshot data loss\n",
    "                phi_s = self.a * tf.reduce_mean(tf.square( self.model_r(self.xs) - self.f(self.xs) ))\n",
    "            \n",
    "            # Add phi_b, phi_i, and phi_s to the loss\n",
    "            loss += phi_b + phi_i + phi_s\n",
    "\n",
    "                \n",
    "            return loss, phi_r, phi_b, phi_i, phi_s\n",
    "        \n",
    "\n",
    "        def get_gradient(self, x):\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                # This tape is for derivatives with respect to trainable variables\n",
    "                tape.watch(self.model_r.trainable_variables)\n",
    "                loss, _, _, _, _ = self.loss_function(x)\n",
    "\n",
    "            g = tape.gradient(loss, self.model_r.trainable_variables)\n",
    "            del tape\n",
    "\n",
    "            return g\n",
    "\n",
    "\n",
    "        def solve(self, optimizer, batch_size, numEpochs):\n",
    "\n",
    "            @tf.function\n",
    "            def train_step(x):\n",
    "                # Retrieve loss gradient w.r.t. trainable variables\n",
    "                grad_theta = self.get_gradient(x)\n",
    "\n",
    "                # Perform gradient descent step\n",
    "                optimizer.apply_gradients(zip(grad_theta, self.model_r.trainable_variables))\n",
    "            \n",
    "            # Split data into training batches\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((self.x,))\n",
    "            train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "            \n",
    "            # Iterate training\n",
    "            for i in range(numEpochs):\n",
    "                \n",
    "                # Train on each batch\n",
    "                for (x_batch_train,) in train_dataset:\n",
    "                    train_step(x_batch_train)\n",
    "                \n",
    "                # Compute loss for full dataset to track training progress\n",
    "                self.loss, self.phi_r, self.phi_b, self.phi_i, self.phi_s = self.loss_function(self.x)\n",
    "                \n",
    "    \n",
    "    # Return intialized class instance\n",
    "    return PINN_Solver_Schwarz(model_r, model_i, X_r, X_b, alpha, snap, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5468d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Multiple domain Schwarz Decomposition of PINNs with strongly enforced dirichlet boundary conditions\n",
    "\n",
    "# Shell function used for dynamic class inheritance of PDEs\n",
    "def PINN_SDBC_Schwarz_Steady(model_r, model_i, X_r, X_b, alpha, pde, snap=0, **kwargs):\n",
    "    \n",
    "    class PINN_Solver_Schwarz(pde):\n",
    "        def __init__(self, model_r, model_i, X_r, X_b, alpha, snap, **kwargs):\n",
    "            \n",
    "            # Intialize dynamic superclass with its default parameter signature\n",
    "            super().__init__(**kwargs)\n",
    "            \n",
    "            # Store models \n",
    "            self.model_r = model_r\n",
    "            self.model_i = model_i\n",
    "\n",
    "            # Store internal collocation points\n",
    "            self.x = X_r\n",
    "\n",
    "            # Store boundary points\n",
    "            self.xb = X_b\n",
    "            \n",
    "            # Store multiplier for BC enforcement function\n",
    "            self.m = 5\n",
    "            \n",
    "            # Store snapshot points if applicable\n",
    "            if snap:\n",
    "                self.xs = tf.constant(np.linspace(float(self.xb[0][0][0]), float(self.xb[1][0][0]), num=snap, \n",
    "                                    endpoint=False)[1:], shape=(snap-1, 1), dtype='float64')\n",
    "\n",
    "            # Store loss scaling coefficient\n",
    "            self.a = alpha\n",
    "        \n",
    "        def BC_enforce(self, x):\n",
    "            return (tf.math.tanh( self.m*(1-x) )*tf.math.tanh( x ))\n",
    "        \n",
    "        def get_residual(self, x):\n",
    "\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                # Watch variable x during this GradientTape\n",
    "                tape.watch(x)\n",
    "                \n",
    "                # Compute current values u(x) with strongly enforced BCs\n",
    "                u = self.BC_enforce(x)*self.model_r(x)\n",
    "                \n",
    "                # Store first derivative\n",
    "                u_x = tape.gradient(u, x)\n",
    "            \n",
    "            # Store second derivative \n",
    "            u_xx = tape.gradient(u_x, x)\n",
    "            del tape\n",
    "\n",
    "            return self.f_r(u_x, u_xx)\n",
    "        \n",
    "\n",
    "        def loss_function(self, x):\n",
    "\n",
    "            # Compute phi_r\n",
    "            r = self.get_residual(x)\n",
    "            phi_r = self.a * tf.reduce_mean(tf.square(r))\n",
    "\n",
    "            # Initialize loss with residual loss function\n",
    "            loss = phi_r\n",
    "                \n",
    "            i=0\n",
    "            phi_i = 0\n",
    "            for b,y in self.xb:\n",
    "                if y:\n",
    "                    continue\n",
    "            \n",
    "                # Calculate interface loss for current model if applicable\n",
    "                u_pred1 = self.BC_enforce(b)*self.model_r(b)\n",
    "                u_pred2 = self.BC_enforce(b)*self.model_i[i](b)\n",
    "                phi_i += (1 - self.a) * tf.reduce_mean(tf.square(u_pred1 - u_pred2))\n",
    "                i += 1\n",
    "            \n",
    "            phi_s = 0\n",
    "            if snap:\n",
    "                # calculate snapshot data loss\n",
    "                phi_s = (1 - self.a) * tf.reduce_mean(tf.square( self.BC_enforce(self.xs)*self.model_r(self.xs) - self.f(self.xs) ))\n",
    "            \n",
    "            # Add phi_b, phi_i, and phi_s to the loss\n",
    "            loss += phi_i + phi_s\n",
    "\n",
    "                \n",
    "            return loss, phi_r, phi_i, phi_s\n",
    "        \n",
    "\n",
    "        def get_gradient(self, x):\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                # This tape is for derivatives with respect to trainable variables\n",
    "                tape.watch(self.model_r.trainable_variables)\n",
    "                loss, _, _, _ = self.loss_function(x)\n",
    "\n",
    "            g = tape.gradient(loss, self.model_r.trainable_variables)\n",
    "            del tape\n",
    "\n",
    "            return g\n",
    "\n",
    "\n",
    "        def solve(self, optimizer, batch_size, numEpochs):\n",
    "\n",
    "            @tf.function\n",
    "            def train_step(x):\n",
    "                # Retrieve loss gradient w.r.t. trainable variables\n",
    "                grad_theta = self.get_gradient(x)\n",
    "\n",
    "                # Perform gradient descent step\n",
    "                optimizer.apply_gradients(zip(grad_theta, self.model_r.trainable_variables))\n",
    "            \n",
    "            # Split data into training batches\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((self.x,))\n",
    "            train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "            \n",
    "            # Iterate training\n",
    "            for i in range(numEpochs):\n",
    "                \n",
    "                # Train on each batch\n",
    "                for (x_batch_train,) in train_dataset:\n",
    "                    train_step(x_batch_train)\n",
    "                \n",
    "                # Compute loss for full dataset to track training progress\n",
    "                self.loss, self.phi_r, self.phi_i, self.phi_s = self.loss_function(self.x)\n",
    "                \n",
    "    \n",
    "    # Return intialized class instance\n",
    "    return PINN_Solver_Schwarz(model_r, model_i, X_r, X_b, alpha, snap, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac35ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10be855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Multiple domain Schwarz Coupling of a PINN with a finite difference model using both SDBCs and WDBCs\n",
    "\n",
    "# Shell function used for dynamic class inheritance of PDEs\n",
    "def FD_PINN_Schwarz_Steady(model_r, u_int, X_r, X_b, alpha, pde, strong, snap=0, **kwargs):\n",
    "    \n",
    "    class PINN_Solver_Schwarz(pde):\n",
    "        def __init__(self, model_r, u_int, X_r, X_b, alpha, strong, snap, **kwargs):\n",
    "            \n",
    "            # Intialize dynamic superclass with its default parameter signature\n",
    "            super().__init__(**kwargs)\n",
    "            \n",
    "            # Store models \n",
    "            self.model_r = model_r\n",
    "            self.u = u_int\n",
    "\n",
    "            # Store internal collocation points\n",
    "            self.x = X_r\n",
    "\n",
    "            # Store boundary points\n",
    "            self.xb = X_b\n",
    "            \n",
    "            # Store multiplier for BC enforcement function\n",
    "            self.m = 5\n",
    "            \n",
    "            # Store snapshot points if applicable\n",
    "            if snap:\n",
    "                self.xs = tf.constant(np.linspace(float(self.xb[0][0][0]), float(self.xb[1][0][0]), num=snap, \n",
    "                                    endpoint=False)[1:], shape=(snap-1, 1), dtype='float64')\n",
    "\n",
    "            # Store loss scaling coefficient\n",
    "            self.a = alpha\n",
    "        \n",
    "        def BC_enforce(self, x):\n",
    "            return (tf.math.tanh( self.m*(1-x) )*tf.math.tanh( x ))\n",
    "        \n",
    "        def get_residual(self, x):\n",
    "\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                # Watch variable x during this GradientTape\n",
    "                tape.watch(x)\n",
    "                \n",
    "                # Compute current values u(x) with strongly enforced BCs\n",
    "                if strong:\n",
    "                    u = self.BC_enforce(x)*self.model_r(x)\n",
    "                else:\n",
    "                    u = self.model_r(x)\n",
    "                \n",
    "                # Store first derivative\n",
    "                u_x = tape.gradient(u, x)\n",
    "            \n",
    "            # Store second derivative \n",
    "            u_xx = tape.gradient(u_x, x)\n",
    "            del tape\n",
    "\n",
    "            return self.f_r(u_x, u_xx)\n",
    "        \n",
    "\n",
    "        def loss_strong(self, x):\n",
    "\n",
    "            # Compute phi_r\n",
    "            r = self.get_residual(x)\n",
    "            phi_r = self.a * tf.reduce_mean(tf.square(r))\n",
    "\n",
    "            # Initialize loss with residual loss function\n",
    "            loss = phi_r\n",
    "                \n",
    "            i=0\n",
    "            phi_i = 0\n",
    "            for b,y in self.xb:\n",
    "                if y:\n",
    "                    continue\n",
    "            \n",
    "                # Calculate interface loss for current model if applicable\n",
    "                u_pred1 = self.BC_enforce(b)*self.model_r(b)\n",
    "                phi_i += (1 - self.a) * tf.reduce_mean(tf.square(u_pred1 - self.u))\n",
    "                i += 1\n",
    "            \n",
    "            phi_s = 0\n",
    "            if snap:\n",
    "                # calculate snapshot data loss\n",
    "                phi_s = (1 - self.a) * tf.reduce_mean(tf.square( self.BC_enforce(self.xs)*self.model_r(self.xs) - self.f(self.xs) ))\n",
    "            \n",
    "            # Add phi_b, phi_i, and phi_s to the loss\n",
    "            loss += phi_i + phi_s\n",
    "            \n",
    "            return loss, phi_r, phi_i, phi_s\n",
    "        \n",
    "        def loss_weak(self, x):\n",
    "\n",
    "            # Compute phi_r\n",
    "            r = self.get_residual(x)\n",
    "            phi_r = self.a * tf.reduce_mean(tf.square(r))\n",
    "\n",
    "            # Initialize loss with residual loss function\n",
    "            loss = phi_r\n",
    "            \n",
    "            i=0\n",
    "            phi_b = 0\n",
    "            phi_i = 0\n",
    "            for b,y in self.xb:\n",
    "                # Calculate boundary loss for current model if applicable\n",
    "                if y:\n",
    "                    u_pred = self.model_r(b)\n",
    "                    phi_b += (1 - self.a) * tf.reduce_mean(tf.square(self.f_b(b) - u_pred))\n",
    "                    continue\n",
    "            \n",
    "                # Calculate interface loss for current model if applicable\n",
    "                u_pred1 = self.model_r(b)\n",
    "                phi_i += (1 - self.a) * tf.reduce_mean(tf.square(u_pred1 - self.u))\n",
    "                i += 1\n",
    "            \n",
    "            phi_s = 0\n",
    "            if snap:\n",
    "                # calculate snapshot data loss\n",
    "                phi_s = self.a * tf.reduce_mean(tf.square( self.model_r(self.xs) - self.f(self.xs) ))\n",
    "            \n",
    "            # Add phi_b, phi_i, and phi_s to the loss\n",
    "            loss += phi_b + phi_i + phi_s\n",
    "            \n",
    "            return loss, phi_r, phi_b, phi_i, phi_s\n",
    "\n",
    "        def get_gradient(self, x):\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                # This tape is for derivatives with respect to trainable variables\n",
    "                tape.watch(self.model_r.trainable_variables)\n",
    "                if strong:\n",
    "                    loss, _, _, _ = self.loss_strong(x)\n",
    "                else:\n",
    "                    loss, _, _, _, _ = self.loss_weak(x)\n",
    "\n",
    "            g = tape.gradient(loss, self.model_r.trainable_variables)\n",
    "            del tape\n",
    "\n",
    "            return g\n",
    "\n",
    "\n",
    "        def solve(self, optimizer, batch_size, numEpochs):\n",
    "\n",
    "            @tf.function\n",
    "            def train_step(x):\n",
    "                # Retrieve loss gradient w.r.t. trainable variables\n",
    "                grad_theta = self.get_gradient(x)\n",
    "\n",
    "                # Perform gradient descent step\n",
    "                optimizer.apply_gradients(zip(grad_theta, self.model_r.trainable_variables))\n",
    "            \n",
    "            # Split data into training batches\n",
    "            train_dataset = tf.data.Dataset.from_tensor_slices((self.x,))\n",
    "            train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "            \n",
    "            # Iterate training\n",
    "            for i in range(numEpochs):\n",
    "                \n",
    "                # Train on each batch\n",
    "                for (x_batch_train,) in train_dataset:\n",
    "                    train_step(x_batch_train)\n",
    "                \n",
    "                # Compute loss for full dataset to track training progress\n",
    "                if strong:\n",
    "                    self.loss, self.phi_r, self.phi_i, self.phi_s = self.loss_strong(self.x)\n",
    "                else:\n",
    "                    self.loss, self.phi_r, self.phi_b, self.phi_i, self.phi_s = self.loss_weak(self.x)\n",
    "    \n",
    "    # Return intialized class instance\n",
    "    return PINN_Solver_Schwarz(model_r, u_int, X_r, X_b, alpha, strong, snap, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222765e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandia",
   "language": "python",
   "name": "sandia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
