{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c7bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nbimporter\n",
    "from PDE_Classes import *\n",
    "\n",
    "# A general class for constructing neural network architecture\n",
    "class PINN_Architecture(tf.keras.Model):\n",
    "    def __init__(self, xl=0, xr=1,\n",
    "            num_hidden_layers=2, \n",
    "            num_neurons_per_layer=20,\n",
    "            output_dim=1,\n",
    "            activation=tf.keras.activations.swish,\n",
    "            kernel_initializer='glorot_normal',\n",
    "            bias_initializer=tf.keras.initializers.RandomNormal(mean=0.4, stddev=0.1, seed=42),\n",
    "            **kwargs):\n",
    "        \n",
    "        # Intialize superclass with its default parameter signature\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Store model subdomain bounds\n",
    "        self.xl = xl\n",
    "        self.xr = xr\n",
    "        \n",
    "        # Initialize storage for BCs according to adjacent models\n",
    "        self.u_gamma = np.array([0, 0])\n",
    "        \n",
    "        # Store hyperparameters\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Define NN architecture\n",
    "        self.hidden = [tf.keras.layers.Dense(num_neurons_per_layer,\n",
    "                             activation=tf.keras.activations.get(activation),\n",
    "                             kernel_initializer=kernel_initializer)#, \n",
    "                             #bias_initializer=bias_initializer)\n",
    "                           for _ in range(self.num_hidden_layers)]\n",
    "        self.out = tf.keras.layers.Dense(output_dim)\n",
    "        \n",
    "    \n",
    "    # Mimic functionality of model(x)\n",
    "    def call(self, X):\n",
    "        #Forward-pass through neural network.\n",
    "        Z = self.hidden[0](X)\n",
    "        for i in range(1, self.num_hidden_layers):\n",
    "            Z = self.hidden[i](Z)\n",
    "        return self.out(Z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b741a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A general class for FD models on subdomains\n",
    "class FD_1D_Steady():\n",
    "    def __init__(self, X, BC, pde):\n",
    "\n",
    "        self.X = X\n",
    "        n_FD = len(X)\n",
    "        xl = X[0]\n",
    "        xr = X[-1]\n",
    "        \n",
    "        h = X[1]-X[0]\n",
    "        \n",
    "        nu = pde.nu\n",
    "        beta = pde.beta\n",
    "        order = pde.order\n",
    "        \n",
    "        a = - nu/(h**2)\n",
    "        b = (2*nu)/(h**2)\n",
    "        c = -(nu/(h**2))\n",
    "\n",
    "        if order == 1:\n",
    "            b += beta / h\n",
    "            c += -beta / h\n",
    "            d = 0.0\n",
    "        elif order == 2:\n",
    "            b += 3 / 2 * beta / h\n",
    "            c += -2 * beta / h\n",
    "            d = 1 / 2 * beta / h\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid order: {order}\")\n",
    "        \n",
    "        A = np.diagflat([b]*(n_FD)) + np.diagflat([c]*(n_FD - 1), -1) + np.diagflat([a]*(n_FD - 1), 1) + np.diagflat([d] * (n_FD - 2), -2)\n",
    "\n",
    "        if xr == BC[1]:\n",
    "            y = np.ones((A.shape[0]-1, 1)) \n",
    "            y[0] += -d*np.random.rand(1) - c*np.random.rand(1)\n",
    "            y[1] += -d*np.random.rand(1)\n",
    "\n",
    "            u_FD = np.linalg.solve( A[:-1, :-1], y )\n",
    "            self.u = np.hstack( (u_FD.flatten(), pde.f(xr)) )\n",
    "\n",
    "        elif xl == BC[0]:\n",
    "            y = np.ones((A.shape[0]-1, 1))\n",
    "            y[-1] += -a*np.random.rand(1)\n",
    "\n",
    "            u_FD = np.linalg.solve( A[:-1, :-1], y )\n",
    "            self.u = np.hstack( (pde.f(xl), u_FD.flatten()) )\n",
    "\n",
    "        else:\n",
    "            y = np.ones((A.shape[0], 1))\n",
    "            y[0] += -d*np.random.rand(1) - c*np.random.rand(1)\n",
    "            y[1] += -d*np.random.rand(1)\n",
    "            y[-1] += -a*np.random.rand(1)\n",
    "\n",
    "            u_FD = np.linalg.solve( A, y )\n",
    "            self.u = np.squeeze(u_FD)\n",
    "        \n",
    "        # save FD matrices for updating model\n",
    "        self.A = A\n",
    "        self.coeff = (a, b, c, d)\n",
    "        \n",
    "        # Initialize storage for BCs according to adjacent models\n",
    "        self.u_gamma = np.array([0, 0])\n",
    "        \n",
    "    # Mimic functionality of model(x)\n",
    "    def __call__(self, x):\n",
    "        return np.interp(x, self.X, self.u)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fc60d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN_Schwarz_Steady():\n",
    "    def __init__(self, pde, model_r, model_i, SDBC, X_r, X_b, alpha, snap):\n",
    "        \n",
    "        # Store PDE\n",
    "        self.pde = pde\n",
    "        \n",
    "        # Store models \n",
    "        self.model_r = model_r\n",
    "        self.model_i = model_i\n",
    "        \n",
    "        # Check for SDBC enforcement\n",
    "        self.sdbc = SDBC\n",
    "\n",
    "        # Store internal collocation points\n",
    "        self.x = X_r\n",
    "\n",
    "        # Store boundary points\n",
    "        self.xb = X_b\n",
    "        \n",
    "        # Store snapshot points if applicable\n",
    "        if snap:\n",
    "            npxs = np.linspace(float(model_r.xl), float(model_r.xr), num=snap, \n",
    "                                endpoint=False)[1:]\n",
    "            self.xs = tf.constant(npxs, shape=(snap-1, 1), dtype='float64')\n",
    "            \n",
    "        self.snap = snap\n",
    "        \n",
    "        # Store loss scaling coefficient\n",
    "        self.a = alpha\n",
    "        \n",
    "        # scalar modifier for tanh enforcement functions\n",
    "        self.m = 1\n",
    "        \n",
    "        # NN model loss and FD error storage\n",
    "        self.loss = 0\n",
    "        self.err = 0\n",
    "    \n",
    "    \n",
    "    def get_u_hat(self, x, model):\n",
    "    \n",
    "        # Check if only schwarz boundaries are enforced\n",
    "        if not self.sdbc[0]:\n",
    "            bounds = [model.xl, model.xr]\n",
    "            \n",
    "            # Scaling functions to selectively enforce NN outputs to 0 at boundaries\n",
    "            v_lr = [tf.math.tanh(self.m*(x - bounds[0])), tf.math.tanh(self.m*(bounds[1] - x))]\n",
    "            \n",
    "            # scaling functions to smooth enforcement of Schwarz BCs\n",
    "            scale = [10**(-10*(x-bounds[0])), 10**(10*(x-bounds[1]))]\n",
    "            \n",
    "            # determine v_x based on boundary type of input model\n",
    "            v_x = 1\n",
    "            for i,b in enumerate(bounds):\n",
    "                if not (b==0 or b==1):\n",
    "                    v_x *= v_lr[i]\n",
    "            \n",
    "            # scale model outputs\n",
    "            u_hat = v_x*model(x)\n",
    "            \n",
    "            # add BCs according to adjacent models\n",
    "            for i,u_g in enumerate(model.u_gamma): \n",
    "                u_hat = u_hat + scale[i]*u_g\n",
    "            \n",
    "            return u_hat\n",
    "\n",
    "        # Check if only system boundaries are enforced\n",
    "        elif not self.sdbc[1]:\n",
    "            return tf.math.tanh(self.m*(1 - x))*tf.math.tanh(self.m*x)*model(x)\n",
    "        \n",
    "        # Else both are strong boundaries\n",
    "        else:\n",
    "            bounds = [model.xl, model.xr]\n",
    "            \n",
    "            # Scaling function to enforce NN outputs to 0 at all boundaries\n",
    "            v_x = tf.math.tanh(self.m*(x - bounds[0]))*tf.math.tanh(self.m*(bounds[1] - x))\n",
    "            \n",
    "            # scaling functions to smooth enforcement of Schwarz BCs\n",
    "            scale = [10**(-10*(x-bounds[0])), 10**(10*(x-bounds[1]))]\n",
    "            \n",
    "            # scale model outputs\n",
    "            u_hat = v_x*model(x)\n",
    "            \n",
    "            # add BCs according to adjacent models\n",
    "            for i,u_g in enumerate(model.u_gamma): \n",
    "                u_hat = u_hat + scale[i]*u_g\n",
    "            \n",
    "            return u_hat\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def get_residual(self, x):\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Watch variable x during this GradientTape\n",
    "            tape.watch(x)\n",
    "\n",
    "            # Compute current values u(x) with strongly enforced BCs\n",
    "            if any(self.sdbc):\n",
    "                u = self.get_u_hat(x, self.model_r)\n",
    "            else:\n",
    "                u = self.model_r(x)\n",
    "\n",
    "            # Store first derivative\n",
    "            u_x = tape.gradient(u, x)\n",
    "            \n",
    "        # Store second derivative \n",
    "        u_xx = tape.gradient(u_x, x)\n",
    "\n",
    "        del tape\n",
    "\n",
    "        return self.pde.f_r(u_x, u_xx)\n",
    "    \n",
    "    # Enforce system and/or Schwarz boundaries strongly \n",
    "    def loss_strong(self, x):\n",
    "\n",
    "        # Compute phi_r\n",
    "        r = self.get_residual(x)\n",
    "        phi_r = self.a * tf.reduce_mean(tf.square(r))\n",
    "\n",
    "        # Initialize loss with residual loss function\n",
    "        loss = phi_r\n",
    "        \n",
    "        phi_b = 0\n",
    "        phi_i = 0\n",
    "        \n",
    "        # Check if only system boundaries are enforced\n",
    "        if not self.sdbc[0]: \n",
    "            \n",
    "            # Calculate system boundary loss for current model if applicable\n",
    "            for i,model in enumerate(self.model_i):\n",
    "                if model:\n",
    "                    continue\n",
    "                \n",
    "                b = self.xb[i]\n",
    "                \n",
    "                u_hat = self.get_u_hat(b, self.model_r)\n",
    "\n",
    "                phi_b += (1 - self.a) * tf.reduce_mean(tf.square( self.pde.f_b(b) - u_hat ))      \n",
    "\n",
    "\n",
    "        # Check if only system boundaries are enforced\n",
    "        elif not self.sdbc[1]:\n",
    "            \n",
    "            # Calculate Schwarz boundary loss for current model if applicable\n",
    "            for i,model in enumerate(self.model_i):\n",
    "                if not model:\n",
    "                    continue\n",
    "\n",
    "                b = self.xb[i]\n",
    "                    \n",
    "                u_pred1 = self.get_u_hat(b, self.model_r)\n",
    "                \n",
    "                if isinstance(model[0], FD_1D_Steady):\n",
    "                    u_pred2 = model[0](b)\n",
    "                else:\n",
    "                    u_pred2 = self.get_u_hat(b, model[0])\n",
    "                \n",
    "                phi_i += (1 - self.a) * tf.reduce_mean(tf.square(u_pred1 - u_pred2))\n",
    "                \n",
    "        \n",
    "        phi_s = 0\n",
    "        if self.snap:\n",
    "            # calculate snapshot data loss\n",
    "            phi_s = (1 - self.a) * tf.reduce_mean(tf.square( self.get_u_hat(self.xs, self.model_r) \n",
    "                                                            - self.pde.f(self.xs) ))\n",
    "\n",
    "        # Add phi_b, phi_i, and phi_s to the loss\n",
    "        loss += phi_b + phi_i + phi_s\n",
    "\n",
    "        return loss, phi_r, phi_b, phi_i, phi_s\n",
    "\n",
    "    \n",
    "    # Enforce system and Schwarz boundaries weakly \n",
    "    def loss_weak(self, x):\n",
    "        \n",
    "        # Compute phi_r\n",
    "        r = self.get_residual(x)\n",
    "        phi_r = self.a * tf.reduce_mean(tf.square(r))\n",
    "\n",
    "        # Initialize loss with residual loss function\n",
    "        loss = phi_r\n",
    "\n",
    "        phi_b = 0\n",
    "        phi_i = 0\n",
    "        for i, model in enumerate(self.model_i):\n",
    "\n",
    "            b = self.xb[i]\n",
    "            \n",
    "            # Calculate boundary loss for current model if applicable\n",
    "            if not model:\n",
    "                u_pred = self.model_r(b)\n",
    "                phi_b += (1 - self.a) * tf.reduce_mean(tf.square(self.pde.f_b(b) - u_pred))\n",
    "                continue\n",
    "            \n",
    "            # Calculate interface loss for current model if applicable\n",
    "            u_pred1 = self.model_r(b)\n",
    "            u_pred2 = model[0](b)\n",
    "            phi_i += (1 - self.a) * tf.reduce_mean(tf.square(u_pred1 - u_pred2))\n",
    "\n",
    "        phi_s = 0\n",
    "        if self.snap:\n",
    "            # calculate snapshot data loss\n",
    "            phi_s = (1 - self.a) * tf.reduce_mean(tf.square( self.model_r(self.xs) - self.pde.f(self.xs) ))\n",
    "\n",
    "        # Add phi_b, phi_i, and phi_s to the loss\n",
    "        loss += phi_b + phi_i + phi_s\n",
    "\n",
    "        return loss, phi_r, phi_b, phi_i, phi_s\n",
    "\n",
    "    \n",
    "    @tf.function\n",
    "    def get_gradient(self, x):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # This tape is for derivatives with respect to trainable variables\n",
    "            tape.watch(self.model_r.trainable_variables)\n",
    "            if any(self.sdbc):\n",
    "                loss, _, _, _, _ = self.loss_strong(x)\n",
    "            else:\n",
    "                loss, _, _, _, _ = self.loss_weak(x)\n",
    "\n",
    "        g = tape.gradient(loss, self.model_r.trainable_variables)\n",
    "\n",
    "        return g\n",
    "\n",
    "\n",
    "    def FD_update(self):\n",
    "\n",
    "        a, b, c, d = self.model_r.coeff\n",
    "        model = self.model_i\n",
    "        A = self.model_r.A\n",
    "\n",
    "        if (model[0] and model[1]):\n",
    "            f_NN = np.ones((A.shape[0], 1))\n",
    "\n",
    "            if (any(self.sdbc) and not isinstance(model[0][0], FD_1D_Steady)):\n",
    "                f_NN[0] = f_NN[0] + ( -d*self.model_r.u_gamma[0] \n",
    "                                     - c*self.get_u_hat(tf.reshape(self.x[1], shape=(1,1)),model[0][0]) )\n",
    "                f_NN[1] = f_NN[1] + ( -d*self.get_u_hat(tf.reshape(self.x[1], shape=(1,1)),model[0][0]) )\n",
    "\n",
    "            else:    \n",
    "                f_NN[0] = f_NN[0] + ( -d*model[0][0](tf.reshape(self.x[0], shape=(1,1))) \n",
    "                             -c*model[0][0](tf.reshape(self.x[1], shape=(1,1))) )\n",
    "                f_NN[1] = f_NN[1] + ( -d*model[0][0](tf.reshape(self.x[1], shape=(1,1))) )\n",
    "\n",
    "            if (any(self.sdbc) and not isinstance(model[1][0], FD_1D_Steady)):\n",
    "                f_NN[-1] = f_NN[-1] + ( -a*self.model_r.u_gamma[1] )\n",
    "            else:    \n",
    "                f_NN[-1] = f_NN[-1] + ( -a*model[1][0](tf.reshape(self.x[-1], shape=(1,1))) )\n",
    "\n",
    "            u_FD = np.squeeze( np.linalg.solve( A, f_NN ) )\n",
    "\n",
    "        elif model[1]:\n",
    "            f_NN = np.ones((A.shape[0]-1, 1))\n",
    "            A = A[:-1, :-1]\n",
    "\n",
    "            if (any(self.sdbc) and not isinstance(model[1][0], FD_1D_Steady)):\n",
    "                f_NN[-1] = f_NN[-1] + ( -a*self.model_r.u_gamma[1] )\n",
    "            else:    \n",
    "                f_NN[-1] = f_NN[-1] + ( -a*model[1][0](tf.reshape(self.x[-1], shape=(1,1))) )\n",
    "\n",
    "            u_FD = np.linalg.solve( A, f_NN )\n",
    "\n",
    "            u_FD = np.hstack((self.pde.f(self.x[0]), u_FD.flatten()))\n",
    "\n",
    "        elif model[0]:\n",
    "            f_NN = np.ones((A.shape[0]-1, 1))\n",
    "            A = A[:-1, :-1]\n",
    "\n",
    "            if (any(self.sdbc) and not isinstance(model[0][0], FD_1D_Steady)):\n",
    "                f_NN[0] = f_NN[0] + ( -d*self.model_r.u_gamma[0] \n",
    "                                     - c*self.get_u_hat(tf.reshape(self.x[1], shape=(1,1)),model[0][0]) )\n",
    "                f_NN[1] = f_NN[1] + ( -d*self.get_u_hat(tf.reshape(self.x[1], shape=(1,1)),model[0][0]) )\n",
    "\n",
    "            else:    \n",
    "                f_NN[0] = f_NN[0] + ( -d*model[0][0](tf.reshape(self.x[0], shape=(1,1))) \n",
    "                             -c*model[0][0](tf.reshape(self.x[1], shape=(1,1))) )\n",
    "                f_NN[1] = f_NN[1] + ( -d*model[0][0](tf.reshape(self.x[1], shape=(1,1))) )\n",
    "\n",
    "            u_FD = np.linalg.solve( A, f_NN )\n",
    "\n",
    "            u_FD = np.hstack((u_FD.flatten(), self.pde.f(self.x[-1])))\n",
    "\n",
    "        else:\n",
    "\n",
    "            u_FD = np.squeeze(self.pde.f(self.x))\n",
    "\n",
    "        # Update u for current model\n",
    "        self.model_r.u = u_FD\n",
    "\n",
    "\n",
    "    def solve(self, optimizer, numEpochs):\n",
    "\n",
    "        @tf.function\n",
    "        def train_step(x):\n",
    "            # Retrieve loss gradient w.r.t. trainable variables\n",
    "            grad_theta = self.get_gradient(x)\n",
    "\n",
    "            # Perform gradient descent step\n",
    "            optimizer.apply_gradients(zip(grad_theta, self.model_r.trainable_variables))\n",
    "\n",
    "        # Split data into training batches\n",
    "        #train_dataset = tf.data.Dataset.from_tensor_slices((self.x,))\n",
    "        #train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "\n",
    "        # If current model is FOM, update interface boundaries with adjacent NN models\n",
    "        if isinstance(self.model_r, FD_1D_Steady):\n",
    "            self.FD_update()\n",
    "            self.err = np.square(self.model_r.u - self.pde.f(self.x)).mean()\n",
    "        else: \n",
    "            # Iterate training\n",
    "            for i in range(numEpochs):\n",
    "\n",
    "                # Train on each batch\n",
    "                #for (x_batch_train,) in train_dataset:\n",
    "                    #train_step(x_batch_train)\n",
    "                train_step(self.x)\n",
    "\n",
    "            # Compute loss for full dataset to track training progress\n",
    "            if any(self.sdbc):\n",
    "                self.loss, self.phi_r, self.phi_b, self.phi_i, self.phi_s = self.loss_strong(self.x)\n",
    "            else:\n",
    "                self.loss, self.phi_r, self.phi_b, self.phi_i, self.phi_s = self.loss_weak(self.x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ROM_ML] *",
   "language": "python",
   "name": "conda-env-ROM_ML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
